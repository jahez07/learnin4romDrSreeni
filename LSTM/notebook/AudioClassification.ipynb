{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37195c8-ec5e-4b00-ace1-1e252544930e",
   "metadata": {},
   "source": [
    "# **Audio Classification with LSTM and Torch Audio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e4926-a8e2-40ef-a5f2-4c071692c709",
   "metadata": {},
   "source": [
    "## **Libraries Required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e0c8fc-171a-41fe-92e2-5db6139dbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3d651-e468-4172-a29b-e3a99dabe752",
   "metadata": {},
   "source": [
    "## **About Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01645cd7-b786-4d5e-971a-0577cfaa096d",
   "metadata": {},
   "source": [
    "This dataset contains 8732 labeled sound excerpts ( <= 4s ) of urban sounds from 10 classes :`air_conditioner`, `car_horn`, `children_playing`, `dog_bark`, `drilling`, `engine_idling`, `gun_shot`, `jackhammer`, `siren` and `street_music`. The classes are drawn from the urban sound taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c539fd6e-0d2a-44ca-88da-47d2df1343ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    # Wrapper for the UrbanSound8K dataset\n",
    "    # Argument List\n",
    "    # path to the UrbanSound8K csv file\n",
    "    # path to the UrbanSound8K audio files\n",
    "    # list of folders to use in the dataset\n",
    "\n",
    "    def __init__(self, csv_path, file_path, folderList):\n",
    "        csvData = pd.read_csv(csv_path)\n",
    "        # initialize lists to hold file names, labels, and folder numbers\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        # loop through the csv entries and only add entries from folders in the folder list\n",
    "        for i in range(0, len(csvData)):\n",
    "            if csvData.iloc[i, 5] in folderList:\n",
    "                self.file_names.append(csvData.iloc[i, 0]) # extracting & appending each element from column 1- filenames\n",
    "                self.labels.append(csvData.iloc[i, 6]) # extracting & appending each element from column 7 - classID\n",
    "                self.folders.append(csvData.iloc[i, 5]) # extracting & appending elements from column 6 -folderNumber\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.folderList = folderList\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # format the file path and load the file\n",
    "        path = self.file_path + \"fold\" + str(self.folders[index]) + \"/\" + self.file_names[index]\n",
    "        sound, sample_rate = torchaudio.load(path, normalize=True)\n",
    "        soundData = torch.mean(sound, dim=0, keepdim=True)\n",
    "        tempData = torch.zeros([1, 160000])  # tempData accounts for audio clips that are too short\n",
    "\n",
    "        if soundData.numel() < 160000: # checks the number of elements in soundData tensor\n",
    "            tempData[:, :soundData.numel()] = soundData\n",
    "        else:\n",
    "            tempData = soundData[:, :160000]\n",
    "\n",
    "        soundData = tempData\n",
    "\n",
    "        mel_specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(soundData)  # (channel, n_mels, time)\n",
    "        mel_specgram_norm = (mel_specgram - mel_specgram.mean()) / mel_specgram.std()\n",
    "        mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate)(soundData)  # (channel, n_mfcc, time)\n",
    "        mfcc_norm = (mfcc - mfcc.mean()) / mfcc.std()\n",
    "        \n",
    "        feature = torch.cat([mel_specgram, mfcc], axis=1)\n",
    "        \n",
    "        return feature[0].permute(1, 0), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "976d68f8-1983-45eb-8083-0df504146efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature=5, out_feature=5, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_feature = n_feature\n",
    "\n",
    "        self.lstm = nn.LSTM(self.n_feature, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, out_feature)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x.shape (batch, seq_len, n_features)\n",
    "        l_out, l_hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # out.shape (batch, seq_len, n_hidden*direction)\n",
    "        out = self.dropout(l_out)\n",
    "\n",
    "        # out.shape (batch, out_feature)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, l_hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "300f7744-aed1-4f3f-9f04-1921a92e15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1339e96f-aced-4308-b065-819bc4fe6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    y_pred, y_target = [], []\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n",
    "        \n",
    "        pred = torch.max(output, dim=1).indices\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "        y_pred = y_pred + pred.tolist()\n",
    "        y_target = y_target + target.tolist()\n",
    "        \n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "374d2fa0-c4b6-44fb-89e9-fadde9fd6aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train set size: 7895\n",
      "Test set size: 837\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"lr\": 0.01, \n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"batch_size\": 128, \n",
    "    \"in_feature\": 168, # number of features per timesteps (e.g., MFCCs, spectrogram bins)\n",
    "    \"out_feature\": 10 # number of output units ( e.g., number of classes )\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "csv_path =  '../data/urbansound8k/UrbanSound8K.csv'\n",
    "file_path = '../data/urbansound8k/'\n",
    "\n",
    "train_set = UrbanSoundDataset(csv_path, file_path, range(1, 10))\n",
    "test_set = UrbanSoundDataset(csv_path, file_path, [10])\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ae9b-b898-4b7b-ab96-be8898ed9e31",
   "metadata": {},
   "source": [
    "1. **num_workers**\n",
    "\n",
    "    * What it means: Nu mber of subprocesses to use for data loading.\n",
    "    * `num_workers=0`: data loading is done in the main process (slower).\n",
    "    * `num_workers>0`: multiple worker processes load data in parallel (faster).\n",
    "\n",
    "ðŸ‘‰ Here itâ€™s set to 1, meaning one extra worker process will load data in parallel to the training loop.\n",
    "\n",
    "2. **pin_memory**\n",
    "\n",
    "    * What it means: If True, the data loader will copy tensors into page-locked (pinned) memory.\n",
    "\n",
    "    * Why? When using a GPU (device == 'cuda'), pinned memory allows faster and more efficient transfer of data from CPU â†’ GPU.\n",
    "\n",
    "    * Without it, transfers may be slower because regular memory can be swapped out by the OS.\n",
    "\n",
    "ðŸ‘‰ Itâ€™s only useful when training on GPU. Thatâ€™s why itâ€™s conditionally set only if device == 'cuda'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52d7489e-7c1c-4093-950a-c55bdd68de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}  # needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d7df414-c8fd-4912-b594-e71d1f887a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioLSTM(\n",
      "  (lstm): LSTM(168, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AudioLSTM(\n",
    "    n_feature=hyperparameters[\"in_feature\"],\n",
    "    out_feature=hyperparameters[\"out_feature\"])\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hyperparameters['lr'],\n",
    "    weight_decay=hyperparameters['weight_decay'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "clip = 5  # gradient clipping\n",
    "\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b29a926-89fb-46c0-8387-0215d6d383e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/functional/functional.py:585: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/7895 (0%)]\tLoss: 2.305977\n",
      "Train Epoch: 1 [1280/7895 (16%)]\tLoss: 1.905224\n",
      "Train Epoch: 1 [2560/7895 (33%)]\tLoss: 1.815500\n",
      "Train Epoch: 1 [3840/7895 (49%)]\tLoss: 1.765416\n",
      "Train Epoch: 1 [5120/7895 (66%)]\tLoss: 1.613798\n",
      "Train Epoch: 1 [6400/7895 (82%)]\tLoss: 1.612137\n",
      "Train Epoch: 1 [7680/7895 (98%)]\tLoss: 1.700481\n",
      "\n",
      "Test set: Accuracy: 278/837 (33%)\n",
      "\n",
      "Train Epoch: 2 [0/7895 (0%)]\tLoss: 1.826766\n",
      "Train Epoch: 2 [1280/7895 (16%)]\tLoss: 1.719805\n",
      "Train Epoch: 2 [2560/7895 (33%)]\tLoss: 1.637850\n",
      "Train Epoch: 2 [3840/7895 (49%)]\tLoss: 1.686120\n",
      "Train Epoch: 2 [5120/7895 (66%)]\tLoss: 1.793617\n",
      "Train Epoch: 2 [6400/7895 (82%)]\tLoss: 1.463389\n",
      "Train Epoch: 2 [7680/7895 (98%)]\tLoss: 1.568965\n",
      "\n",
      "Test set: Accuracy: 301/837 (36%)\n",
      "\n",
      "Train Epoch: 3 [0/7895 (0%)]\tLoss: 1.522715\n",
      "Train Epoch: 3 [1280/7895 (16%)]\tLoss: 1.932638\n",
      "Train Epoch: 3 [2560/7895 (33%)]\tLoss: 1.662760\n",
      "Train Epoch: 3 [3840/7895 (49%)]\tLoss: 1.839450\n",
      "Train Epoch: 3 [5120/7895 (66%)]\tLoss: 1.763222\n",
      "Train Epoch: 3 [6400/7895 (82%)]\tLoss: 1.744694\n",
      "Train Epoch: 3 [7680/7895 (98%)]\tLoss: 1.416974\n",
      "\n",
      "Test set: Accuracy: 265/837 (32%)\n",
      "\n",
      "Train Epoch: 4 [0/7895 (0%)]\tLoss: 1.760473\n",
      "Train Epoch: 4 [1280/7895 (16%)]\tLoss: 1.721981\n",
      "Train Epoch: 4 [2560/7895 (33%)]\tLoss: 1.518649\n",
      "Train Epoch: 4 [3840/7895 (49%)]\tLoss: 1.622099\n",
      "Train Epoch: 4 [5120/7895 (66%)]\tLoss: 1.796241\n",
      "Train Epoch: 4 [6400/7895 (82%)]\tLoss: 1.672475\n",
      "Train Epoch: 4 [7680/7895 (98%)]\tLoss: 1.626297\n",
      "\n",
      "Test set: Accuracy: 227/837 (27%)\n",
      "\n",
      "Train Epoch: 5 [0/7895 (0%)]\tLoss: 1.664741\n",
      "Train Epoch: 5 [1280/7895 (16%)]\tLoss: 1.421988\n",
      "Train Epoch: 5 [2560/7895 (33%)]\tLoss: 1.786658\n",
      "Train Epoch: 5 [3840/7895 (49%)]\tLoss: 1.410744\n",
      "Train Epoch: 5 [5120/7895 (66%)]\tLoss: 1.406430\n",
      "Train Epoch: 5 [6400/7895 (82%)]\tLoss: 1.382660\n",
      "Train Epoch: 5 [7680/7895 (98%)]\tLoss: 1.497527\n",
      "\n",
      "Test set: Accuracy: 298/837 (36%)\n",
      "\n",
      "Train Epoch: 6 [0/7895 (0%)]\tLoss: 1.559048\n",
      "Train Epoch: 6 [1280/7895 (16%)]\tLoss: 1.346783\n",
      "Train Epoch: 6 [2560/7895 (33%)]\tLoss: 1.728526\n",
      "Train Epoch: 6 [3840/7895 (49%)]\tLoss: 1.405593\n",
      "Train Epoch: 6 [5120/7895 (66%)]\tLoss: 1.375746\n",
      "Train Epoch: 6 [6400/7895 (82%)]\tLoss: 1.502695\n",
      "Train Epoch: 6 [7680/7895 (98%)]\tLoss: 1.639520\n",
      "\n",
      "Test set: Accuracy: 368/837 (44%)\n",
      "\n",
      "Train Epoch: 7 [0/7895 (0%)]\tLoss: 1.656752\n",
      "Train Epoch: 7 [1280/7895 (16%)]\tLoss: 1.296501\n",
      "Train Epoch: 7 [2560/7895 (33%)]\tLoss: 1.248328\n",
      "Train Epoch: 7 [3840/7895 (49%)]\tLoss: 1.253496\n",
      "Train Epoch: 7 [5120/7895 (66%)]\tLoss: 1.335134\n",
      "Train Epoch: 7 [6400/7895 (82%)]\tLoss: 1.361048\n",
      "Train Epoch: 7 [7680/7895 (98%)]\tLoss: 1.572447\n",
      "\n",
      "Test set: Accuracy: 352/837 (42%)\n",
      "\n",
      "Train Epoch: 8 [0/7895 (0%)]\tLoss: 1.508885\n",
      "Train Epoch: 8 [1280/7895 (16%)]\tLoss: 1.466362\n",
      "Train Epoch: 8 [2560/7895 (33%)]\tLoss: 1.479578\n",
      "Train Epoch: 8 [3840/7895 (49%)]\tLoss: 1.356847\n",
      "Train Epoch: 8 [5120/7895 (66%)]\tLoss: 1.457384\n",
      "Train Epoch: 8 [6400/7895 (82%)]\tLoss: 1.196068\n",
      "Train Epoch: 8 [7680/7895 (98%)]\tLoss: 1.568020\n",
      "\n",
      "Test set: Accuracy: 350/837 (42%)\n",
      "\n",
      "Train Epoch: 9 [0/7895 (0%)]\tLoss: 1.152964\n",
      "Train Epoch: 9 [1280/7895 (16%)]\tLoss: 1.275506\n",
      "Train Epoch: 9 [2560/7895 (33%)]\tLoss: 1.407780\n",
      "Train Epoch: 9 [3840/7895 (49%)]\tLoss: 1.193589\n",
      "Train Epoch: 9 [5120/7895 (66%)]\tLoss: 1.462738\n",
      "Train Epoch: 9 [6400/7895 (82%)]\tLoss: 1.245266\n",
      "Train Epoch: 9 [7680/7895 (98%)]\tLoss: 1.356587\n",
      "\n",
      "Test set: Accuracy: 351/837 (42%)\n",
      "\n",
      "Train Epoch: 10 [0/7895 (0%)]\tLoss: 1.473619\n",
      "Train Epoch: 10 [1280/7895 (16%)]\tLoss: 1.259155\n",
      "Train Epoch: 10 [2560/7895 (33%)]\tLoss: 1.255656\n",
      "Train Epoch: 10 [3840/7895 (49%)]\tLoss: 1.231508\n",
      "Train Epoch: 10 [5120/7895 (66%)]\tLoss: 1.291418\n",
      "Train Epoch: 10 [6400/7895 (82%)]\tLoss: 1.143082\n",
      "Train Epoch: 10 [7680/7895 (98%)]\tLoss: 1.456712\n",
      "\n",
      "Test set: Accuracy: 363/837 (43%)\n",
      "\n",
      "Train Epoch: 11 [0/7895 (0%)]\tLoss: 1.406822\n",
      "Train Epoch: 11 [1280/7895 (16%)]\tLoss: 1.299605\n",
      "Train Epoch: 11 [2560/7895 (33%)]\tLoss: 1.364177\n",
      "Train Epoch: 11 [3840/7895 (49%)]\tLoss: 1.215869\n",
      "Train Epoch: 11 [5120/7895 (66%)]\tLoss: 1.382678\n",
      "Train Epoch: 11 [6400/7895 (82%)]\tLoss: 1.276988\n",
      "Train Epoch: 11 [7680/7895 (98%)]\tLoss: 1.156329\n",
      "\n",
      "Test set: Accuracy: 380/837 (45%)\n",
      "\n",
      "Train Epoch: 12 [0/7895 (0%)]\tLoss: 1.210895\n",
      "Train Epoch: 12 [1280/7895 (16%)]\tLoss: 1.288329\n",
      "Train Epoch: 12 [2560/7895 (33%)]\tLoss: 1.320815\n",
      "Train Epoch: 12 [3840/7895 (49%)]\tLoss: 1.298208\n",
      "Train Epoch: 12 [5120/7895 (66%)]\tLoss: 1.208663\n",
      "Train Epoch: 12 [6400/7895 (82%)]\tLoss: 1.278354\n",
      "Train Epoch: 12 [7680/7895 (98%)]\tLoss: 1.186433\n",
      "\n",
      "Test set: Accuracy: 367/837 (44%)\n",
      "\n",
      "Train Epoch: 13 [0/7895 (0%)]\tLoss: 1.099403\n",
      "Train Epoch: 13 [1280/7895 (16%)]\tLoss: 1.083605\n",
      "Train Epoch: 13 [2560/7895 (33%)]\tLoss: 1.179105\n",
      "Train Epoch: 13 [3840/7895 (49%)]\tLoss: 1.373587\n",
      "Train Epoch: 13 [5120/7895 (66%)]\tLoss: 1.093992\n",
      "Train Epoch: 13 [6400/7895 (82%)]\tLoss: 1.241403\n",
      "Train Epoch: 13 [7680/7895 (98%)]\tLoss: 1.067954\n",
      "\n",
      "Test set: Accuracy: 339/837 (41%)\n",
      "\n",
      "Train Epoch: 14 [0/7895 (0%)]\tLoss: 1.223787\n",
      "Train Epoch: 14 [1280/7895 (16%)]\tLoss: 1.084692\n",
      "Train Epoch: 14 [2560/7895 (33%)]\tLoss: 1.269169\n",
      "Train Epoch: 14 [3840/7895 (49%)]\tLoss: 1.061445\n",
      "Train Epoch: 14 [5120/7895 (66%)]\tLoss: 1.152224\n",
      "Train Epoch: 14 [6400/7895 (82%)]\tLoss: 1.288819\n",
      "Train Epoch: 14 [7680/7895 (98%)]\tLoss: 1.279204\n",
      "\n",
      "Test set: Accuracy: 376/837 (45%)\n",
      "\n",
      "Train Epoch: 15 [0/7895 (0%)]\tLoss: 1.074930\n",
      "Train Epoch: 15 [1280/7895 (16%)]\tLoss: 1.164283\n",
      "Train Epoch: 15 [2560/7895 (33%)]\tLoss: 1.197725\n",
      "Train Epoch: 15 [3840/7895 (49%)]\tLoss: 1.110727\n",
      "Train Epoch: 15 [5120/7895 (66%)]\tLoss: 1.281564\n",
      "Train Epoch: 15 [6400/7895 (82%)]\tLoss: 1.115241\n",
      "Train Epoch: 15 [7680/7895 (98%)]\tLoss: 1.090185\n",
      "\n",
      "Test set: Accuracy: 381/837 (46%)\n",
      "\n",
      "Train Epoch: 16 [0/7895 (0%)]\tLoss: 0.941245\n",
      "Train Epoch: 16 [1280/7895 (16%)]\tLoss: 1.098539\n",
      "Train Epoch: 16 [2560/7895 (33%)]\tLoss: 1.080412\n",
      "Train Epoch: 16 [3840/7895 (49%)]\tLoss: 1.180845\n",
      "Train Epoch: 16 [5120/7895 (66%)]\tLoss: 0.987516\n",
      "Train Epoch: 16 [6400/7895 (82%)]\tLoss: 0.849092\n",
      "Train Epoch: 16 [7680/7895 (98%)]\tLoss: 1.112059\n",
      "\n",
      "Test set: Accuracy: 388/837 (46%)\n",
      "\n",
      "Train Epoch: 17 [0/7895 (0%)]\tLoss: 0.847220\n",
      "Train Epoch: 17 [1280/7895 (16%)]\tLoss: 0.868821\n",
      "Train Epoch: 17 [2560/7895 (33%)]\tLoss: 1.036056\n",
      "Train Epoch: 17 [3840/7895 (49%)]\tLoss: 1.078468\n",
      "Train Epoch: 17 [5120/7895 (66%)]\tLoss: 1.152195\n",
      "Train Epoch: 17 [6400/7895 (82%)]\tLoss: 1.007772\n",
      "Train Epoch: 17 [7680/7895 (98%)]\tLoss: 1.025942\n",
      "\n",
      "Test set: Accuracy: 393/837 (47%)\n",
      "\n",
      "Train Epoch: 18 [0/7895 (0%)]\tLoss: 1.057074\n",
      "Train Epoch: 18 [1280/7895 (16%)]\tLoss: 1.100626\n",
      "Train Epoch: 18 [2560/7895 (33%)]\tLoss: 1.136752\n",
      "Train Epoch: 18 [3840/7895 (49%)]\tLoss: 1.202595\n",
      "Train Epoch: 18 [5120/7895 (66%)]\tLoss: 1.011257\n",
      "Train Epoch: 18 [6400/7895 (82%)]\tLoss: 1.048366\n",
      "Train Epoch: 18 [7680/7895 (98%)]\tLoss: 1.149281\n",
      "\n",
      "Test set: Accuracy: 401/837 (48%)\n",
      "\n",
      "Train Epoch: 19 [0/7895 (0%)]\tLoss: 1.151183\n",
      "Train Epoch: 19 [1280/7895 (16%)]\tLoss: 0.866618\n",
      "Train Epoch: 19 [2560/7895 (33%)]\tLoss: 1.114202\n",
      "Train Epoch: 19 [3840/7895 (49%)]\tLoss: 0.984674\n",
      "Train Epoch: 19 [5120/7895 (66%)]\tLoss: 1.018097\n",
      "Train Epoch: 19 [6400/7895 (82%)]\tLoss: 0.909341\n",
      "Train Epoch: 19 [7680/7895 (98%)]\tLoss: 1.120725\n",
      "\n",
      "Test set: Accuracy: 421/837 (50%)\n",
      "\n",
      "Train Epoch: 20 [0/7895 (0%)]\tLoss: 1.084545\n",
      "Train Epoch: 20 [1280/7895 (16%)]\tLoss: 0.879695\n",
      "Train Epoch: 20 [2560/7895 (33%)]\tLoss: 1.069586\n",
      "Train Epoch: 20 [3840/7895 (49%)]\tLoss: 0.886198\n",
      "Train Epoch: 20 [5120/7895 (66%)]\tLoss: 1.189481\n",
      "Train Epoch: 20 [6400/7895 (82%)]\tLoss: 1.038829\n",
      "Train Epoch: 20 [7680/7895 (98%)]\tLoss: 0.964576\n",
      "\n",
      "Test set: Accuracy: 388/837 (46%)\n",
      "\n",
      "Train Epoch: 21 [0/7895 (0%)]\tLoss: 0.966191\n",
      "Train Epoch: 21 [1280/7895 (16%)]\tLoss: 1.000705\n",
      "Train Epoch: 21 [2560/7895 (33%)]\tLoss: 1.028314\n",
      "Train Epoch: 21 [3840/7895 (49%)]\tLoss: 1.077851\n",
      "Train Epoch: 21 [5120/7895 (66%)]\tLoss: 0.977674\n",
      "Train Epoch: 21 [6400/7895 (82%)]\tLoss: 0.953972\n",
      "Train Epoch: 21 [7680/7895 (98%)]\tLoss: 0.998809\n",
      "\n",
      "Test set: Accuracy: 402/837 (48%)\n",
      "\n",
      "Train Epoch: 22 [0/7895 (0%)]\tLoss: 1.040239\n",
      "Train Epoch: 22 [1280/7895 (16%)]\tLoss: 1.037949\n",
      "Train Epoch: 22 [2560/7895 (33%)]\tLoss: 1.024871\n",
      "Train Epoch: 22 [3840/7895 (49%)]\tLoss: 0.818135\n",
      "Train Epoch: 22 [5120/7895 (66%)]\tLoss: 0.960954\n",
      "Train Epoch: 22 [6400/7895 (82%)]\tLoss: 0.830620\n",
      "Train Epoch: 22 [7680/7895 (98%)]\tLoss: 0.861472\n",
      "\n",
      "Test set: Accuracy: 421/837 (50%)\n",
      "\n",
      "Train Epoch: 23 [0/7895 (0%)]\tLoss: 0.854322\n",
      "Train Epoch: 23 [1280/7895 (16%)]\tLoss: 0.747728\n",
      "Train Epoch: 23 [2560/7895 (33%)]\tLoss: 0.973351\n",
      "Train Epoch: 23 [3840/7895 (49%)]\tLoss: 1.248522\n",
      "Train Epoch: 23 [5120/7895 (66%)]\tLoss: 0.939557\n",
      "Train Epoch: 23 [6400/7895 (82%)]\tLoss: 0.836426\n",
      "Train Epoch: 23 [7680/7895 (98%)]\tLoss: 0.865419\n",
      "\n",
      "Test set: Accuracy: 468/837 (56%)\n",
      "\n",
      "Train Epoch: 24 [0/7895 (0%)]\tLoss: 0.843181\n",
      "Train Epoch: 24 [1280/7895 (16%)]\tLoss: 0.855748\n",
      "Train Epoch: 24 [2560/7895 (33%)]\tLoss: 0.839807\n",
      "Train Epoch: 24 [3840/7895 (49%)]\tLoss: 0.834619\n",
      "Train Epoch: 24 [5120/7895 (66%)]\tLoss: 0.937153\n",
      "Train Epoch: 24 [6400/7895 (82%)]\tLoss: 1.253034\n",
      "Train Epoch: 24 [7680/7895 (98%)]\tLoss: 0.876223\n",
      "\n",
      "Test set: Accuracy: 458/837 (55%)\n",
      "\n",
      "Train Epoch: 25 [0/7895 (0%)]\tLoss: 0.839123\n",
      "Train Epoch: 25 [1280/7895 (16%)]\tLoss: 0.675140\n",
      "Train Epoch: 25 [2560/7895 (33%)]\tLoss: 0.799202\n",
      "Train Epoch: 25 [3840/7895 (49%)]\tLoss: 0.893052\n",
      "Train Epoch: 25 [5120/7895 (66%)]\tLoss: 0.692403\n",
      "Train Epoch: 25 [6400/7895 (82%)]\tLoss: 0.862920\n",
      "Train Epoch: 25 [7680/7895 (98%)]\tLoss: 0.628449\n",
      "\n",
      "Test set: Accuracy: 466/837 (56%)\n",
      "\n",
      "Train Epoch: 26 [0/7895 (0%)]\tLoss: 0.753960\n",
      "Train Epoch: 26 [1280/7895 (16%)]\tLoss: 0.828393\n",
      "Train Epoch: 26 [2560/7895 (33%)]\tLoss: 0.713817\n",
      "Train Epoch: 26 [3840/7895 (49%)]\tLoss: 1.182624\n",
      "Train Epoch: 26 [5120/7895 (66%)]\tLoss: 0.794100\n",
      "Train Epoch: 26 [6400/7895 (82%)]\tLoss: 0.794426\n",
      "Train Epoch: 26 [7680/7895 (98%)]\tLoss: 0.677360\n",
      "\n",
      "Test set: Accuracy: 451/837 (54%)\n",
      "\n",
      "Train Epoch: 27 [0/7895 (0%)]\tLoss: 0.948946\n",
      "Train Epoch: 27 [1280/7895 (16%)]\tLoss: 0.789351\n",
      "Train Epoch: 27 [2560/7895 (33%)]\tLoss: 0.694101\n",
      "Train Epoch: 27 [3840/7895 (49%)]\tLoss: 0.778258\n",
      "Train Epoch: 27 [5120/7895 (66%)]\tLoss: 0.871434\n",
      "Train Epoch: 27 [6400/7895 (82%)]\tLoss: 0.717512\n",
      "Train Epoch: 27 [7680/7895 (98%)]\tLoss: 0.775291\n",
      "\n",
      "Test set: Accuracy: 454/837 (54%)\n",
      "\n",
      "Train Epoch: 28 [0/7895 (0%)]\tLoss: 0.555508\n",
      "Train Epoch: 28 [1280/7895 (16%)]\tLoss: 0.794541\n",
      "Train Epoch: 28 [2560/7895 (33%)]\tLoss: 0.904404\n",
      "Train Epoch: 28 [3840/7895 (49%)]\tLoss: 0.785365\n",
      "Train Epoch: 28 [5120/7895 (66%)]\tLoss: 0.822170\n",
      "Train Epoch: 28 [6400/7895 (82%)]\tLoss: 0.635163\n",
      "Train Epoch: 28 [7680/7895 (98%)]\tLoss: 0.632990\n",
      "\n",
      "Test set: Accuracy: 410/837 (49%)\n",
      "\n",
      "Train Epoch: 29 [0/7895 (0%)]\tLoss: 0.659269\n",
      "Train Epoch: 29 [1280/7895 (16%)]\tLoss: 0.735889\n",
      "Train Epoch: 29 [2560/7895 (33%)]\tLoss: 0.902028\n",
      "Train Epoch: 29 [3840/7895 (49%)]\tLoss: 0.966026\n",
      "Train Epoch: 29 [5120/7895 (66%)]\tLoss: 0.831636\n",
      "Train Epoch: 29 [6400/7895 (82%)]\tLoss: 0.632200\n",
      "Train Epoch: 29 [7680/7895 (98%)]\tLoss: 0.707491\n",
      "\n",
      "Test set: Accuracy: 453/837 (54%)\n",
      "\n",
      "Train Epoch: 30 [0/7895 (0%)]\tLoss: 0.961746\n",
      "Train Epoch: 30 [1280/7895 (16%)]\tLoss: 0.661270\n",
      "Train Epoch: 30 [2560/7895 (33%)]\tLoss: 0.712091\n",
      "Train Epoch: 30 [3840/7895 (49%)]\tLoss: 0.627142\n",
      "Train Epoch: 30 [5120/7895 (66%)]\tLoss: 0.696342\n",
      "Train Epoch: 30 [6400/7895 (82%)]\tLoss: 0.566485\n",
      "Train Epoch: 30 [7680/7895 (98%)]\tLoss: 0.801266\n",
      "\n",
      "Test set: Accuracy: 415/837 (50%)\n",
      "\n",
      "Train Epoch: 31 [0/7895 (0%)]\tLoss: 0.594873\n",
      "Train Epoch: 31 [1280/7895 (16%)]\tLoss: 0.640295\n",
      "Train Epoch: 31 [2560/7895 (33%)]\tLoss: 0.593798\n",
      "Train Epoch: 31 [3840/7895 (49%)]\tLoss: 0.653012\n",
      "Train Epoch: 31 [5120/7895 (66%)]\tLoss: 0.704786\n",
      "Train Epoch: 31 [6400/7895 (82%)]\tLoss: 0.591249\n",
      "Train Epoch: 31 [7680/7895 (98%)]\tLoss: 0.914552\n",
      "\n",
      "Test set: Accuracy: 430/837 (51%)\n",
      "\n",
      "Train Epoch: 32 [0/7895 (0%)]\tLoss: 0.694271\n",
      "Train Epoch: 32 [1280/7895 (16%)]\tLoss: 0.788366\n",
      "Train Epoch: 32 [2560/7895 (33%)]\tLoss: 0.750192\n",
      "Train Epoch: 32 [3840/7895 (49%)]\tLoss: 0.826074\n",
      "Train Epoch: 32 [5120/7895 (66%)]\tLoss: 0.628854\n",
      "Train Epoch: 32 [6400/7895 (82%)]\tLoss: 0.657293\n",
      "Train Epoch: 32 [7680/7895 (98%)]\tLoss: 0.664971\n",
      "\n",
      "Test set: Accuracy: 458/837 (55%)\n",
      "\n",
      "Train Epoch: 33 [0/7895 (0%)]\tLoss: 0.619210\n",
      "Train Epoch: 33 [1280/7895 (16%)]\tLoss: 0.667774\n",
      "Train Epoch: 33 [2560/7895 (33%)]\tLoss: 0.776214\n",
      "Train Epoch: 33 [3840/7895 (49%)]\tLoss: 0.742734\n",
      "Train Epoch: 33 [5120/7895 (66%)]\tLoss: 0.743975\n",
      "Train Epoch: 33 [6400/7895 (82%)]\tLoss: 0.589600\n",
      "Train Epoch: 33 [7680/7895 (98%)]\tLoss: 0.620017\n",
      "\n",
      "Test set: Accuracy: 467/837 (56%)\n",
      "\n",
      "Train Epoch: 34 [0/7895 (0%)]\tLoss: 0.658701\n",
      "Train Epoch: 34 [1280/7895 (16%)]\tLoss: 0.553484\n",
      "Train Epoch: 34 [2560/7895 (33%)]\tLoss: 0.674723\n",
      "Train Epoch: 34 [3840/7895 (49%)]\tLoss: 0.733083\n",
      "Train Epoch: 34 [5120/7895 (66%)]\tLoss: 0.746057\n",
      "Train Epoch: 34 [6400/7895 (82%)]\tLoss: 0.577430\n",
      "Train Epoch: 34 [7680/7895 (98%)]\tLoss: 0.774799\n",
      "\n",
      "Test set: Accuracy: 456/837 (54%)\n",
      "\n",
      "Train Epoch: 35 [0/7895 (0%)]\tLoss: 0.702120\n",
      "Train Epoch: 35 [1280/7895 (16%)]\tLoss: 0.683441\n",
      "Train Epoch: 35 [2560/7895 (33%)]\tLoss: 0.627517\n",
      "Train Epoch: 35 [3840/7895 (49%)]\tLoss: 0.508309\n",
      "Train Epoch: 35 [5120/7895 (66%)]\tLoss: 0.604113\n",
      "Train Epoch: 35 [6400/7895 (82%)]\tLoss: 0.883813\n",
      "Train Epoch: 35 [7680/7895 (98%)]\tLoss: 0.560924\n",
      "\n",
      "Test set: Accuracy: 471/837 (56%)\n",
      "\n",
      "Train Epoch: 36 [0/7895 (0%)]\tLoss: 0.595981\n",
      "Train Epoch: 36 [1280/7895 (16%)]\tLoss: 0.528200\n",
      "Train Epoch: 36 [2560/7895 (33%)]\tLoss: 0.507796\n",
      "Train Epoch: 36 [3840/7895 (49%)]\tLoss: 0.873170\n",
      "Train Epoch: 36 [5120/7895 (66%)]\tLoss: 0.871866\n",
      "Train Epoch: 36 [6400/7895 (82%)]\tLoss: 0.939397\n",
      "Train Epoch: 36 [7680/7895 (98%)]\tLoss: 0.692448\n",
      "\n",
      "Test set: Accuracy: 491/837 (59%)\n",
      "\n",
      "Train Epoch: 37 [0/7895 (0%)]\tLoss: 0.727811\n",
      "Train Epoch: 37 [1280/7895 (16%)]\tLoss: 0.628832\n",
      "Train Epoch: 37 [2560/7895 (33%)]\tLoss: 0.643173\n",
      "Train Epoch: 37 [3840/7895 (49%)]\tLoss: 0.750621\n",
      "Train Epoch: 37 [5120/7895 (66%)]\tLoss: 0.884910\n",
      "Train Epoch: 37 [6400/7895 (82%)]\tLoss: 0.710397\n",
      "Train Epoch: 37 [7680/7895 (98%)]\tLoss: 0.580830\n",
      "\n",
      "Test set: Accuracy: 478/837 (57%)\n",
      "\n",
      "Train Epoch: 38 [0/7895 (0%)]\tLoss: 0.643544\n",
      "Train Epoch: 38 [1280/7895 (16%)]\tLoss: 0.438190\n",
      "Train Epoch: 38 [2560/7895 (33%)]\tLoss: 0.457557\n",
      "Train Epoch: 38 [3840/7895 (49%)]\tLoss: 0.532323\n",
      "Train Epoch: 38 [5120/7895 (66%)]\tLoss: 0.405104\n",
      "Train Epoch: 38 [6400/7895 (82%)]\tLoss: 0.745185\n",
      "Train Epoch: 38 [7680/7895 (98%)]\tLoss: 0.587444\n",
      "\n",
      "Test set: Accuracy: 443/837 (53%)\n",
      "\n",
      "Train Epoch: 39 [0/7895 (0%)]\tLoss: 0.459188\n",
      "Train Epoch: 39 [1280/7895 (16%)]\tLoss: 0.632588\n",
      "Train Epoch: 39 [2560/7895 (33%)]\tLoss: 0.490540\n",
      "Train Epoch: 39 [3840/7895 (49%)]\tLoss: 0.785014\n",
      "Train Epoch: 39 [5120/7895 (66%)]\tLoss: 0.685968\n",
      "Train Epoch: 39 [6400/7895 (82%)]\tLoss: 0.527592\n",
      "Train Epoch: 39 [7680/7895 (98%)]\tLoss: 0.614243\n",
      "\n",
      "Test set: Accuracy: 473/837 (57%)\n",
      "\n",
      "Train Epoch: 40 [0/7895 (0%)]\tLoss: 0.721017\n",
      "Train Epoch: 40 [1280/7895 (16%)]\tLoss: 0.756197\n",
      "Train Epoch: 40 [2560/7895 (33%)]\tLoss: 0.662539\n",
      "Train Epoch: 40 [3840/7895 (49%)]\tLoss: 0.658706\n",
      "Train Epoch: 40 [5120/7895 (66%)]\tLoss: 0.596186\n",
      "Train Epoch: 40 [6400/7895 (82%)]\tLoss: 0.802618\n",
      "Train Epoch: 40 [7680/7895 (98%)]\tLoss: 0.560491\n",
      "\n",
      "Test set: Accuracy: 464/837 (55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 41):\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9474f9-81f4-411e-b4c9-63f6e4afc28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnAI",
   "language": "python",
   "name": "learnai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
