{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37195c8-ec5e-4b00-ace1-1e252544930e",
   "metadata": {},
   "source": [
    "# **Audio Classification with LSTM and Torch Audio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e4926-a8e2-40ef-a5f2-4c071692c709",
   "metadata": {},
   "source": [
    "## **Libraries Required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e0c8fc-171a-41fe-92e2-5db6139dbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3d651-e468-4172-a29b-e3a99dabe752",
   "metadata": {},
   "source": [
    "## **About Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01645cd7-b786-4d5e-971a-0577cfaa096d",
   "metadata": {},
   "source": [
    "This dataset contains 8732 labeled sound excerpts ( <= 4s ) of urban sounds from 10 classes :`air_conditioner`, `car_horn`, `children_playing`, `dog_bark`, `drilling`, `engine_idling`, `gun_shot`, `jackhammer`, `siren` and `street_music`. The classes are drawn from the urban sound taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c539fd6e-0d2a-44ca-88da-47d2df1343ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    # Wrapper for the UrbanSound8K dataset\n",
    "    # Argument List\n",
    "    # path to the UrbanSound8K csv file\n",
    "    # path to the UrbanSound8K audio files\n",
    "    # list of folders to use in the dataset\n",
    "\n",
    "    def __init__(self, csv_path, file_path, folderList):\n",
    "        csvData = pd.read_csv(csv_path)\n",
    "        # initialize lists to hold file names, labels, and folder numbers\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        # loop through the csv entries and only add entries from folders in the folder list\n",
    "        for i in range(0, len(csvData)):\n",
    "            if csvData.iloc[i, 5] in folderList:\n",
    "                self.file_names.append(csvData.iloc[i, 0]) # extracting & appending each element from column 1- filenames\n",
    "                self.labels.append(csvData.iloc[i, 6]) # extracting & appending each element from column 7 - classID\n",
    "                self.folders.append(csvData.iloc[i, 5]) # extracting & appending elements from column 6 -folderNumber\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.folderList = folderList\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # format the file path and load the file\n",
    "        path = self.file_path + \"fold\" + str(self.folders[index]) + \"/\" + self.file_names[index]\n",
    "        sound, sample_rate = torchaudio.load(path, normalize=True)\n",
    "        soundData = torch.mean(sound, dim=0, keepdim=True)\n",
    "        tempData = torch.zeros([1, 160000])  # tempData accounts for audio clips that are too short\n",
    "\n",
    "        if soundData.numel() < 160000: # checks the number of elements in soundData tensor\n",
    "            tempData[:, :soundData.numel()] = soundData\n",
    "        else:\n",
    "            tempData = soundData[:, :160000]\n",
    "\n",
    "        soundData = tempData\n",
    "\n",
    "        mel_specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(soundData)  # (channel, n_mels, time)\n",
    "        mel_specgram_norm = (mel_specgram - mel_specgram.mean()) / mel_specgram.std()\n",
    "        mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate)(soundData)  # (channel, n_mfcc, time)\n",
    "        mfcc_norm = (mfcc - mfcc.mean()) / mfcc.std()\n",
    "        \n",
    "        feature = torch.cat([mel_specgram, mfcc], axis=1)\n",
    "        \n",
    "        return feature[0].permute(1, 0), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "976d68f8-1983-45eb-8083-0df504146efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature=5, out_feature=5, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_feature = n_feature\n",
    "\n",
    "        self.lstm = nn.LSTM(self.n_feature, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, out_feature)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x.shape (batch, seq_len, n_features)\n",
    "        l_out, l_hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # out.shape (batch, seq_len, n_hidden*direction)\n",
    "        out = self.dropout(l_out)\n",
    "\n",
    "        # out.shape (batch, out_feature)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, l_hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "300f7744-aed1-4f3f-9f04-1921a92e15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1339e96f-aced-4308-b065-819bc4fe6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    y_pred, y_target = [], []\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n",
    "        \n",
    "        pred = torch.max(output, dim=1).indices\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "        y_pred = y_pred + pred.tolist()\n",
    "        y_target = y_target + target.tolist()\n",
    "        \n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "374d2fa0-c4b6-44fb-89e9-fadde9fd6aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train set size: 7895\n",
      "Test set size: 837\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"lr\": 0.01, \n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"batch_size\": 128, \n",
    "    \"in_feature\": 168, # number of features per timesteps (e.g., MFCCs, spectrogram bins)\n",
    "    \"out_feature\": 10 # number of output units ( e.g., number of classes )\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "csv_path =  '../data/urbansound8k/UrbanSound8K.csv'\n",
    "file_path = '../data/urbansound8k/'\n",
    "\n",
    "train_set = UrbanSoundDataset(csv_path, file_path, range(1, 10))\n",
    "test_set = UrbanSoundDataset(csv_path, file_path, [10])\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ae9b-b898-4b7b-ab96-be8898ed9e31",
   "metadata": {},
   "source": [
    "1. **num_workers**\n",
    "\n",
    "    * What it means: Nu mber of subprocesses to use for data loading.\n",
    "    * `num_workers=0`: data loading is done in the main process (slower).\n",
    "    * `num_workers>0`: multiple worker processes load data in parallel (faster).\n",
    "\n",
    "ðŸ‘‰ Here itâ€™s set to 1, meaning one extra worker process will load data in parallel to the training loop.\n",
    "\n",
    "2. **pin_memory**\n",
    "\n",
    "    * What it means: If True, the data loader will copy tensors into page-locked (pinned) memory.\n",
    "\n",
    "    * Why? When using a GPU (device == 'cuda'), pinned memory allows faster and more efficient transfer of data from CPU â†’ GPU.\n",
    "\n",
    "    * Without it, transfers may be slower because regular memory can be swapped out by the OS.\n",
    "\n",
    "ðŸ‘‰ Itâ€™s only useful when training on GPU. Thatâ€™s why itâ€™s conditionally set only if device == 'cuda'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52d7489e-7c1c-4093-950a-c55bdd68de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}  # needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d7df414-c8fd-4912-b594-e71d1f887a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioLSTM(\n",
      "  (lstm): LSTM(168, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AudioLSTM(\n",
    "    n_feature=hyperparameters[\"in_feature\"],\n",
    "    out_feature=hyperparameters[\"out_feature\"])\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hyperparameters['lr'],\n",
    "    weight_decay=hyperparameters['weight_decay'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "clip = 5  # gradient clipping\n",
    "\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b29a926-89fb-46c0-8387-0215d6d383e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/functional/functional.py:585: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/7895 (0%)]\tLoss: 0.599352\n",
      "Train Epoch: 1 [1280/7895 (16%)]\tLoss: 0.677366\n",
      "Train Epoch: 1 [2560/7895 (33%)]\tLoss: 0.722687\n",
      "Train Epoch: 1 [3840/7895 (49%)]\tLoss: 0.673987\n",
      "Train Epoch: 1 [5120/7895 (66%)]\tLoss: 0.638204\n",
      "Train Epoch: 1 [6400/7895 (82%)]\tLoss: 0.523225\n",
      "Train Epoch: 1 [7680/7895 (98%)]\tLoss: 0.698279\n",
      "\n",
      "Test set: Accuracy: 405/837 (48%)\n",
      "\n",
      "Train Epoch: 2 [0/7895 (0%)]\tLoss: 0.598212\n",
      "Train Epoch: 2 [1280/7895 (16%)]\tLoss: 0.549298\n",
      "Train Epoch: 2 [2560/7895 (33%)]\tLoss: 0.802927\n",
      "Train Epoch: 2 [3840/7895 (49%)]\tLoss: 0.807823\n",
      "Train Epoch: 2 [5120/7895 (66%)]\tLoss: 0.665938\n",
      "Train Epoch: 2 [6400/7895 (82%)]\tLoss: 0.538181\n",
      "Train Epoch: 2 [7680/7895 (98%)]\tLoss: 0.557494\n",
      "\n",
      "Test set: Accuracy: 435/837 (52%)\n",
      "\n",
      "Train Epoch: 3 [0/7895 (0%)]\tLoss: 0.419262\n",
      "Train Epoch: 3 [1280/7895 (16%)]\tLoss: 0.612148\n",
      "Train Epoch: 3 [2560/7895 (33%)]\tLoss: 0.427465\n",
      "Train Epoch: 3 [3840/7895 (49%)]\tLoss: 0.600089\n",
      "Train Epoch: 3 [5120/7895 (66%)]\tLoss: 0.844609\n",
      "Train Epoch: 3 [6400/7895 (82%)]\tLoss: 0.603541\n",
      "Train Epoch: 3 [7680/7895 (98%)]\tLoss: 0.728674\n",
      "\n",
      "Test set: Accuracy: 428/837 (51%)\n",
      "\n",
      "Train Epoch: 4 [0/7895 (0%)]\tLoss: 0.628674\n",
      "Train Epoch: 4 [1280/7895 (16%)]\tLoss: 0.723746\n",
      "Train Epoch: 4 [2560/7895 (33%)]\tLoss: 0.574527\n",
      "Train Epoch: 4 [3840/7895 (49%)]\tLoss: 0.506232\n",
      "Train Epoch: 4 [5120/7895 (66%)]\tLoss: 0.626905\n",
      "Train Epoch: 4 [6400/7895 (82%)]\tLoss: 0.519443\n",
      "Train Epoch: 4 [7680/7895 (98%)]\tLoss: 0.521402\n",
      "\n",
      "Test set: Accuracy: 469/837 (56%)\n",
      "\n",
      "Train Epoch: 5 [0/7895 (0%)]\tLoss: 0.567755\n",
      "Train Epoch: 5 [1280/7895 (16%)]\tLoss: 0.427190\n",
      "Train Epoch: 5 [2560/7895 (33%)]\tLoss: 0.485328\n",
      "Train Epoch: 5 [3840/7895 (49%)]\tLoss: 0.497415\n",
      "Train Epoch: 5 [5120/7895 (66%)]\tLoss: 0.395355\n",
      "Train Epoch: 5 [6400/7895 (82%)]\tLoss: 0.485471\n",
      "Train Epoch: 5 [7680/7895 (98%)]\tLoss: 0.733623\n",
      "\n",
      "Test set: Accuracy: 452/837 (54%)\n",
      "\n",
      "Train Epoch: 6 [0/7895 (0%)]\tLoss: 0.513293\n",
      "Train Epoch: 6 [1280/7895 (16%)]\tLoss: 0.825324\n",
      "Train Epoch: 6 [2560/7895 (33%)]\tLoss: 0.594453\n",
      "Train Epoch: 6 [3840/7895 (49%)]\tLoss: 0.642357\n",
      "Train Epoch: 6 [5120/7895 (66%)]\tLoss: 0.486878\n",
      "Train Epoch: 6 [6400/7895 (82%)]\tLoss: 0.410661\n",
      "Train Epoch: 6 [7680/7895 (98%)]\tLoss: 0.606388\n",
      "\n",
      "Test set: Accuracy: 472/837 (56%)\n",
      "\n",
      "Train Epoch: 7 [0/7895 (0%)]\tLoss: 0.661794\n",
      "Train Epoch: 7 [1280/7895 (16%)]\tLoss: 0.479759\n",
      "Train Epoch: 7 [2560/7895 (33%)]\tLoss: 0.689103\n",
      "Train Epoch: 7 [3840/7895 (49%)]\tLoss: 0.592369\n",
      "Train Epoch: 7 [5120/7895 (66%)]\tLoss: 0.912184\n",
      "Train Epoch: 7 [6400/7895 (82%)]\tLoss: 0.828059\n",
      "Train Epoch: 7 [7680/7895 (98%)]\tLoss: 0.519274\n",
      "\n",
      "Test set: Accuracy: 436/837 (52%)\n",
      "\n",
      "Train Epoch: 8 [0/7895 (0%)]\tLoss: 0.641200\n",
      "Train Epoch: 8 [1280/7895 (16%)]\tLoss: 0.547832\n",
      "Train Epoch: 8 [2560/7895 (33%)]\tLoss: 0.677092\n",
      "Train Epoch: 8 [3840/7895 (49%)]\tLoss: 0.549778\n",
      "Train Epoch: 8 [5120/7895 (66%)]\tLoss: 0.761166\n",
      "Train Epoch: 8 [6400/7895 (82%)]\tLoss: 0.614531\n",
      "Train Epoch: 8 [7680/7895 (98%)]\tLoss: 0.651718\n",
      "\n",
      "Test set: Accuracy: 407/837 (49%)\n",
      "\n",
      "Train Epoch: 9 [0/7895 (0%)]\tLoss: 0.559939\n",
      "Train Epoch: 9 [1280/7895 (16%)]\tLoss: 0.809543\n",
      "Train Epoch: 9 [2560/7895 (33%)]\tLoss: 0.667527\n",
      "Train Epoch: 9 [3840/7895 (49%)]\tLoss: 0.664808\n",
      "Train Epoch: 9 [5120/7895 (66%)]\tLoss: 0.528205\n",
      "Train Epoch: 9 [6400/7895 (82%)]\tLoss: 0.405917\n",
      "Train Epoch: 9 [7680/7895 (98%)]\tLoss: 0.592695\n",
      "\n",
      "Test set: Accuracy: 438/837 (52%)\n",
      "\n",
      "Train Epoch: 10 [0/7895 (0%)]\tLoss: 0.709787\n",
      "Train Epoch: 10 [1280/7895 (16%)]\tLoss: 0.583438\n",
      "Train Epoch: 10 [2560/7895 (33%)]\tLoss: 0.497625\n",
      "Train Epoch: 10 [3840/7895 (49%)]\tLoss: 0.440323\n",
      "Train Epoch: 10 [5120/7895 (66%)]\tLoss: 0.404420\n",
      "Train Epoch: 10 [6400/7895 (82%)]\tLoss: 0.570512\n",
      "Train Epoch: 10 [7680/7895 (98%)]\tLoss: 0.523178\n",
      "\n",
      "Test set: Accuracy: 461/837 (55%)\n",
      "\n",
      "Train Epoch: 11 [0/7895 (0%)]\tLoss: 0.499649\n",
      "Train Epoch: 11 [1280/7895 (16%)]\tLoss: 0.519252\n",
      "Train Epoch: 11 [2560/7895 (33%)]\tLoss: 0.445124\n",
      "Train Epoch: 11 [3840/7895 (49%)]\tLoss: 0.385982\n",
      "Train Epoch: 11 [5120/7895 (66%)]\tLoss: 0.479970\n",
      "Train Epoch: 11 [6400/7895 (82%)]\tLoss: 0.390558\n",
      "Train Epoch: 11 [7680/7895 (98%)]\tLoss: 0.727008\n",
      "\n",
      "Test set: Accuracy: 405/837 (48%)\n",
      "\n",
      "Train Epoch: 12 [0/7895 (0%)]\tLoss: 0.573671\n",
      "Train Epoch: 12 [1280/7895 (16%)]\tLoss: 0.589280\n",
      "Train Epoch: 12 [2560/7895 (33%)]\tLoss: 0.532393\n",
      "Train Epoch: 12 [3840/7895 (49%)]\tLoss: 0.822351\n",
      "Train Epoch: 12 [5120/7895 (66%)]\tLoss: 0.502795\n",
      "Train Epoch: 12 [6400/7895 (82%)]\tLoss: 0.461880\n",
      "Train Epoch: 12 [7680/7895 (98%)]\tLoss: 0.481619\n",
      "\n",
      "Test set: Accuracy: 413/837 (49%)\n",
      "\n",
      "Train Epoch: 13 [0/7895 (0%)]\tLoss: 0.424231\n",
      "Train Epoch: 13 [1280/7895 (16%)]\tLoss: 0.666780\n",
      "Train Epoch: 13 [2560/7895 (33%)]\tLoss: 0.458325\n",
      "Train Epoch: 13 [3840/7895 (49%)]\tLoss: 0.407513\n",
      "Train Epoch: 13 [5120/7895 (66%)]\tLoss: 0.423567\n",
      "Train Epoch: 13 [6400/7895 (82%)]\tLoss: 0.498968\n",
      "Train Epoch: 13 [7680/7895 (98%)]\tLoss: 0.386430\n",
      "\n",
      "Test set: Accuracy: 474/837 (57%)\n",
      "\n",
      "Train Epoch: 14 [0/7895 (0%)]\tLoss: 0.498362\n",
      "Train Epoch: 14 [1280/7895 (16%)]\tLoss: 0.548372\n",
      "Train Epoch: 14 [2560/7895 (33%)]\tLoss: 0.522535\n",
      "Train Epoch: 14 [3840/7895 (49%)]\tLoss: 0.399583\n",
      "Train Epoch: 14 [5120/7895 (66%)]\tLoss: 0.342923\n",
      "Train Epoch: 14 [6400/7895 (82%)]\tLoss: 0.522721\n",
      "Train Epoch: 14 [7680/7895 (98%)]\tLoss: 0.300520\n",
      "\n",
      "Test set: Accuracy: 411/837 (49%)\n",
      "\n",
      "Train Epoch: 15 [0/7895 (0%)]\tLoss: 0.467454\n",
      "Train Epoch: 15 [1280/7895 (16%)]\tLoss: 0.526328\n",
      "Train Epoch: 15 [2560/7895 (33%)]\tLoss: 0.407509\n",
      "Train Epoch: 15 [3840/7895 (49%)]\tLoss: 0.506385\n",
      "Train Epoch: 15 [5120/7895 (66%)]\tLoss: 0.592617\n",
      "Train Epoch: 15 [6400/7895 (82%)]\tLoss: 0.579601\n",
      "Train Epoch: 15 [7680/7895 (98%)]\tLoss: 0.336279\n",
      "\n",
      "Test set: Accuracy: 437/837 (52%)\n",
      "\n",
      "Train Epoch: 16 [0/7895 (0%)]\tLoss: 0.442356\n",
      "Train Epoch: 16 [1280/7895 (16%)]\tLoss: 0.361143\n",
      "Train Epoch: 16 [2560/7895 (33%)]\tLoss: 0.513681\n",
      "Train Epoch: 16 [3840/7895 (49%)]\tLoss: 0.335561\n",
      "Train Epoch: 16 [5120/7895 (66%)]\tLoss: 0.407991\n",
      "Train Epoch: 16 [6400/7895 (82%)]\tLoss: 0.321582\n",
      "Train Epoch: 16 [7680/7895 (98%)]\tLoss: 0.575966\n",
      "\n",
      "Test set: Accuracy: 467/837 (56%)\n",
      "\n",
      "Train Epoch: 17 [0/7895 (0%)]\tLoss: 0.478786\n",
      "Train Epoch: 17 [1280/7895 (16%)]\tLoss: 0.494317\n",
      "Train Epoch: 17 [2560/7895 (33%)]\tLoss: 0.391944\n",
      "Train Epoch: 17 [3840/7895 (49%)]\tLoss: 0.398935\n",
      "Train Epoch: 17 [5120/7895 (66%)]\tLoss: 0.482322\n",
      "Train Epoch: 17 [6400/7895 (82%)]\tLoss: 0.481330\n",
      "Train Epoch: 17 [7680/7895 (98%)]\tLoss: 0.326782\n",
      "\n",
      "Test set: Accuracy: 415/837 (50%)\n",
      "\n",
      "Train Epoch: 18 [0/7895 (0%)]\tLoss: 0.536485\n",
      "Train Epoch: 18 [1280/7895 (16%)]\tLoss: 0.508912\n",
      "Train Epoch: 18 [2560/7895 (33%)]\tLoss: 0.458624\n",
      "Train Epoch: 18 [3840/7895 (49%)]\tLoss: 0.294400\n",
      "Train Epoch: 18 [5120/7895 (66%)]\tLoss: 0.570526\n",
      "Train Epoch: 18 [6400/7895 (82%)]\tLoss: 0.535105\n",
      "Train Epoch: 18 [7680/7895 (98%)]\tLoss: 0.401580\n",
      "\n",
      "Test set: Accuracy: 441/837 (53%)\n",
      "\n",
      "Train Epoch: 19 [0/7895 (0%)]\tLoss: 0.524254\n",
      "Train Epoch: 19 [1280/7895 (16%)]\tLoss: 0.854896\n",
      "Train Epoch: 19 [2560/7895 (33%)]\tLoss: 0.617315\n",
      "Train Epoch: 19 [3840/7895 (49%)]\tLoss: 0.381009\n",
      "Train Epoch: 19 [5120/7895 (66%)]\tLoss: 0.507248\n",
      "Train Epoch: 19 [6400/7895 (82%)]\tLoss: 0.453897\n",
      "Train Epoch: 19 [7680/7895 (98%)]\tLoss: 0.555739\n",
      "\n",
      "Test set: Accuracy: 392/837 (47%)\n",
      "\n",
      "Train Epoch: 20 [0/7895 (0%)]\tLoss: 0.479596\n",
      "Train Epoch: 20 [1280/7895 (16%)]\tLoss: 0.601292\n",
      "Train Epoch: 20 [2560/7895 (33%)]\tLoss: 0.406534\n",
      "Train Epoch: 20 [3840/7895 (49%)]\tLoss: 0.517716\n",
      "Train Epoch: 20 [5120/7895 (66%)]\tLoss: 0.546709\n",
      "Train Epoch: 20 [6400/7895 (82%)]\tLoss: 0.365321\n",
      "Train Epoch: 20 [7680/7895 (98%)]\tLoss: 0.536283\n",
      "\n",
      "Test set: Accuracy: 409/837 (49%)\n",
      "\n",
      "Train Epoch: 21 [0/7895 (0%)]\tLoss: 0.432256\n",
      "Train Epoch: 21 [1280/7895 (16%)]\tLoss: 0.416191\n",
      "Train Epoch: 21 [2560/7895 (33%)]\tLoss: 0.555245\n",
      "Train Epoch: 21 [3840/7895 (49%)]\tLoss: 0.468645\n",
      "Train Epoch: 21 [5120/7895 (66%)]\tLoss: 0.327335\n",
      "Train Epoch: 21 [6400/7895 (82%)]\tLoss: 0.361137\n",
      "Train Epoch: 21 [7680/7895 (98%)]\tLoss: 0.380956\n",
      "\n",
      "Test set: Accuracy: 433/837 (52%)\n",
      "\n",
      "Train Epoch: 22 [0/7895 (0%)]\tLoss: 0.423472\n",
      "Train Epoch: 22 [1280/7895 (16%)]\tLoss: 0.430184\n",
      "Train Epoch: 22 [2560/7895 (33%)]\tLoss: 0.594445\n",
      "Train Epoch: 22 [3840/7895 (49%)]\tLoss: 0.669656\n",
      "Train Epoch: 22 [5120/7895 (66%)]\tLoss: 0.490602\n",
      "Train Epoch: 22 [6400/7895 (82%)]\tLoss: 0.400875\n",
      "Train Epoch: 22 [7680/7895 (98%)]\tLoss: 0.313067\n",
      "\n",
      "Test set: Accuracy: 477/837 (57%)\n",
      "\n",
      "Train Epoch: 23 [0/7895 (0%)]\tLoss: 0.512661\n",
      "Train Epoch: 23 [1280/7895 (16%)]\tLoss: 0.262349\n",
      "Train Epoch: 23 [2560/7895 (33%)]\tLoss: 0.477366\n",
      "Train Epoch: 23 [3840/7895 (49%)]\tLoss: 0.310098\n",
      "Train Epoch: 23 [5120/7895 (66%)]\tLoss: 0.482247\n",
      "Train Epoch: 23 [6400/7895 (82%)]\tLoss: 0.692988\n",
      "Train Epoch: 23 [7680/7895 (98%)]\tLoss: 0.491129\n",
      "\n",
      "Test set: Accuracy: 477/837 (57%)\n",
      "\n",
      "Train Epoch: 24 [0/7895 (0%)]\tLoss: 0.512608\n",
      "Train Epoch: 24 [1280/7895 (16%)]\tLoss: 0.486517\n",
      "Train Epoch: 24 [2560/7895 (33%)]\tLoss: 0.391100\n",
      "Train Epoch: 24 [3840/7895 (49%)]\tLoss: 0.423041\n",
      "Train Epoch: 24 [5120/7895 (66%)]\tLoss: 0.233181\n",
      "Train Epoch: 24 [6400/7895 (82%)]\tLoss: 0.286409\n",
      "Train Epoch: 24 [7680/7895 (98%)]\tLoss: 0.558677\n",
      "\n",
      "Test set: Accuracy: 456/837 (54%)\n",
      "\n",
      "Train Epoch: 25 [0/7895 (0%)]\tLoss: 0.675918\n",
      "Train Epoch: 25 [1280/7895 (16%)]\tLoss: 0.243568\n",
      "Train Epoch: 25 [2560/7895 (33%)]\tLoss: 0.323723\n",
      "Train Epoch: 25 [3840/7895 (49%)]\tLoss: 0.253587\n",
      "Train Epoch: 25 [5120/7895 (66%)]\tLoss: 0.431027\n",
      "Train Epoch: 25 [6400/7895 (82%)]\tLoss: 0.590550\n",
      "Train Epoch: 25 [7680/7895 (98%)]\tLoss: 0.656393\n",
      "\n",
      "Test set: Accuracy: 402/837 (48%)\n",
      "\n",
      "Train Epoch: 26 [0/7895 (0%)]\tLoss: 0.405459\n",
      "Train Epoch: 26 [1280/7895 (16%)]\tLoss: 0.478541\n",
      "Train Epoch: 26 [2560/7895 (33%)]\tLoss: 0.365376\n",
      "Train Epoch: 26 [3840/7895 (49%)]\tLoss: 0.468512\n",
      "Train Epoch: 26 [5120/7895 (66%)]\tLoss: 0.465825\n",
      "Train Epoch: 26 [6400/7895 (82%)]\tLoss: 0.368250\n",
      "Train Epoch: 26 [7680/7895 (98%)]\tLoss: 0.299407\n",
      "\n",
      "Test set: Accuracy: 416/837 (50%)\n",
      "\n",
      "Train Epoch: 27 [0/7895 (0%)]\tLoss: 0.578558\n",
      "Train Epoch: 27 [1280/7895 (16%)]\tLoss: 0.441593\n",
      "Train Epoch: 27 [2560/7895 (33%)]\tLoss: 0.473772\n",
      "Train Epoch: 27 [3840/7895 (49%)]\tLoss: 0.369882\n",
      "Train Epoch: 27 [5120/7895 (66%)]\tLoss: 0.409435\n",
      "Train Epoch: 27 [6400/7895 (82%)]\tLoss: 0.397407\n",
      "Train Epoch: 27 [7680/7895 (98%)]\tLoss: 0.518608\n",
      "\n",
      "Test set: Accuracy: 470/837 (56%)\n",
      "\n",
      "Train Epoch: 28 [0/7895 (0%)]\tLoss: 0.308525\n",
      "Train Epoch: 28 [1280/7895 (16%)]\tLoss: 0.375423\n",
      "Train Epoch: 28 [2560/7895 (33%)]\tLoss: 0.365188\n",
      "Train Epoch: 28 [3840/7895 (49%)]\tLoss: 0.570341\n",
      "Train Epoch: 28 [5120/7895 (66%)]\tLoss: 0.500338\n",
      "Train Epoch: 28 [6400/7895 (82%)]\tLoss: 0.394263\n",
      "Train Epoch: 28 [7680/7895 (98%)]\tLoss: 0.415208\n",
      "\n",
      "Test set: Accuracy: 465/837 (56%)\n",
      "\n",
      "Train Epoch: 29 [0/7895 (0%)]\tLoss: 0.532395\n",
      "Train Epoch: 29 [1280/7895 (16%)]\tLoss: 0.498327\n",
      "Train Epoch: 29 [2560/7895 (33%)]\tLoss: 0.237498\n",
      "Train Epoch: 29 [3840/7895 (49%)]\tLoss: 0.602207\n",
      "Train Epoch: 29 [5120/7895 (66%)]\tLoss: 0.627790\n",
      "Train Epoch: 29 [6400/7895 (82%)]\tLoss: 0.410301\n",
      "Train Epoch: 29 [7680/7895 (98%)]\tLoss: 0.221628\n",
      "\n",
      "Test set: Accuracy: 454/837 (54%)\n",
      "\n",
      "Train Epoch: 30 [0/7895 (0%)]\tLoss: 0.382054\n",
      "Train Epoch: 30 [1280/7895 (16%)]\tLoss: 0.517368\n",
      "Train Epoch: 30 [2560/7895 (33%)]\tLoss: 0.278618\n",
      "Train Epoch: 30 [3840/7895 (49%)]\tLoss: 0.339677\n",
      "Train Epoch: 30 [5120/7895 (66%)]\tLoss: 0.331150\n",
      "Train Epoch: 30 [6400/7895 (82%)]\tLoss: 0.575423\n",
      "Train Epoch: 30 [7680/7895 (98%)]\tLoss: 0.563547\n",
      "\n",
      "Test set: Accuracy: 464/837 (55%)\n",
      "\n",
      "Train Epoch: 31 [0/7895 (0%)]\tLoss: 0.453264\n",
      "Train Epoch: 31 [1280/7895 (16%)]\tLoss: 0.268894\n",
      "Train Epoch: 31 [2560/7895 (33%)]\tLoss: 0.531860\n",
      "Train Epoch: 31 [3840/7895 (49%)]\tLoss: 0.397013\n",
      "Train Epoch: 31 [5120/7895 (66%)]\tLoss: 0.775464\n",
      "Train Epoch: 31 [6400/7895 (82%)]\tLoss: 0.323080\n",
      "Train Epoch: 31 [7680/7895 (98%)]\tLoss: 0.460720\n",
      "\n",
      "Test set: Accuracy: 407/837 (49%)\n",
      "\n",
      "Train Epoch: 32 [0/7895 (0%)]\tLoss: 0.504611\n",
      "Train Epoch: 32 [1280/7895 (16%)]\tLoss: 0.318582\n",
      "Train Epoch: 32 [2560/7895 (33%)]\tLoss: 0.333230\n",
      "Train Epoch: 32 [3840/7895 (49%)]\tLoss: 0.363372\n",
      "Train Epoch: 32 [5120/7895 (66%)]\tLoss: 0.604683\n",
      "Train Epoch: 32 [6400/7895 (82%)]\tLoss: 0.354923\n",
      "Train Epoch: 32 [7680/7895 (98%)]\tLoss: 0.473296\n",
      "\n",
      "Test set: Accuracy: 466/837 (56%)\n",
      "\n",
      "Train Epoch: 33 [0/7895 (0%)]\tLoss: 0.333744\n",
      "Train Epoch: 33 [1280/7895 (16%)]\tLoss: 0.373751\n",
      "Train Epoch: 33 [2560/7895 (33%)]\tLoss: 0.277518\n",
      "Train Epoch: 33 [3840/7895 (49%)]\tLoss: 0.477961\n",
      "Train Epoch: 33 [5120/7895 (66%)]\tLoss: 0.532126\n",
      "Train Epoch: 33 [6400/7895 (82%)]\tLoss: 0.407584\n",
      "Train Epoch: 33 [7680/7895 (98%)]\tLoss: 0.511613\n",
      "\n",
      "Test set: Accuracy: 378/837 (45%)\n",
      "\n",
      "Train Epoch: 34 [0/7895 (0%)]\tLoss: 0.299425\n",
      "Train Epoch: 34 [1280/7895 (16%)]\tLoss: 0.442931\n",
      "Train Epoch: 34 [2560/7895 (33%)]\tLoss: 0.367678\n",
      "Train Epoch: 34 [3840/7895 (49%)]\tLoss: 0.294857\n",
      "Train Epoch: 34 [5120/7895 (66%)]\tLoss: 0.245187\n",
      "Train Epoch: 34 [6400/7895 (82%)]\tLoss: 0.270756\n",
      "Train Epoch: 34 [7680/7895 (98%)]\tLoss: 0.357322\n",
      "\n",
      "Test set: Accuracy: 420/837 (50%)\n",
      "\n",
      "Train Epoch: 35 [0/7895 (0%)]\tLoss: 0.413099\n",
      "Train Epoch: 35 [1280/7895 (16%)]\tLoss: 0.296880\n",
      "Train Epoch: 35 [2560/7895 (33%)]\tLoss: 0.469041\n",
      "Train Epoch: 35 [3840/7895 (49%)]\tLoss: 0.542181\n",
      "Train Epoch: 35 [5120/7895 (66%)]\tLoss: 0.346795\n",
      "Train Epoch: 35 [6400/7895 (82%)]\tLoss: 0.502758\n",
      "Train Epoch: 35 [7680/7895 (98%)]\tLoss: 0.356609\n",
      "\n",
      "Test set: Accuracy: 408/837 (49%)\n",
      "\n",
      "Train Epoch: 36 [0/7895 (0%)]\tLoss: 0.446260\n",
      "Train Epoch: 36 [1280/7895 (16%)]\tLoss: 0.350986\n",
      "Train Epoch: 36 [2560/7895 (33%)]\tLoss: 0.415250\n",
      "Train Epoch: 36 [3840/7895 (49%)]\tLoss: 0.453102\n",
      "Train Epoch: 36 [5120/7895 (66%)]\tLoss: 0.399050\n",
      "Train Epoch: 36 [6400/7895 (82%)]\tLoss: 0.473175\n",
      "Train Epoch: 36 [7680/7895 (98%)]\tLoss: 0.388369\n",
      "\n",
      "Test set: Accuracy: 418/837 (50%)\n",
      "\n",
      "Train Epoch: 37 [0/7895 (0%)]\tLoss: 0.361697\n",
      "Train Epoch: 37 [1280/7895 (16%)]\tLoss: 0.396714\n",
      "Train Epoch: 37 [2560/7895 (33%)]\tLoss: 0.392398\n",
      "Train Epoch: 37 [3840/7895 (49%)]\tLoss: 0.353802\n",
      "Train Epoch: 37 [5120/7895 (66%)]\tLoss: 0.366277\n",
      "Train Epoch: 37 [6400/7895 (82%)]\tLoss: 0.395442\n",
      "Train Epoch: 37 [7680/7895 (98%)]\tLoss: 0.455389\n",
      "\n",
      "Test set: Accuracy: 413/837 (49%)\n",
      "\n",
      "Train Epoch: 38 [0/7895 (0%)]\tLoss: 0.371688\n",
      "Train Epoch: 38 [1280/7895 (16%)]\tLoss: 0.297541\n",
      "Train Epoch: 38 [2560/7895 (33%)]\tLoss: 0.259881\n",
      "Train Epoch: 38 [3840/7895 (49%)]\tLoss: 0.472128\n",
      "Train Epoch: 38 [5120/7895 (66%)]\tLoss: 0.310027\n",
      "Train Epoch: 38 [6400/7895 (82%)]\tLoss: 0.364106\n",
      "Train Epoch: 38 [7680/7895 (98%)]\tLoss: 0.439853\n",
      "\n",
      "Test set: Accuracy: 447/837 (53%)\n",
      "\n",
      "Train Epoch: 39 [0/7895 (0%)]\tLoss: 0.388481\n",
      "Train Epoch: 39 [1280/7895 (16%)]\tLoss: 0.471776\n",
      "Train Epoch: 39 [2560/7895 (33%)]\tLoss: 0.421597\n",
      "Train Epoch: 39 [3840/7895 (49%)]\tLoss: 0.543288\n",
      "Train Epoch: 39 [5120/7895 (66%)]\tLoss: 0.431213\n",
      "Train Epoch: 39 [6400/7895 (82%)]\tLoss: 0.365395\n",
      "Train Epoch: 39 [7680/7895 (98%)]\tLoss: 0.426132\n",
      "\n",
      "Test set: Accuracy: 469/837 (56%)\n",
      "\n",
      "Train Epoch: 40 [0/7895 (0%)]\tLoss: 0.390031\n",
      "Train Epoch: 40 [1280/7895 (16%)]\tLoss: 0.438677\n",
      "Train Epoch: 40 [2560/7895 (33%)]\tLoss: 0.423422\n",
      "Train Epoch: 40 [3840/7895 (49%)]\tLoss: 0.346137\n",
      "Train Epoch: 40 [5120/7895 (66%)]\tLoss: 0.326325\n",
      "Train Epoch: 40 [6400/7895 (82%)]\tLoss: 0.370807\n",
      "Train Epoch: 40 [7680/7895 (98%)]\tLoss: 0.321342\n",
      "\n",
      "Test set: Accuracy: 476/837 (57%)\n",
      "\n",
      "Train Epoch: 41 [0/7895 (0%)]\tLoss: 0.338472\n",
      "Train Epoch: 41 [1280/7895 (16%)]\tLoss: 0.368797\n",
      "Train Epoch: 41 [2560/7895 (33%)]\tLoss: 0.328827\n",
      "Train Epoch: 41 [3840/7895 (49%)]\tLoss: 0.297087\n",
      "Train Epoch: 41 [5120/7895 (66%)]\tLoss: 0.397490\n",
      "Train Epoch: 41 [6400/7895 (82%)]\tLoss: 0.520212\n",
      "Train Epoch: 41 [7680/7895 (98%)]\tLoss: 0.408547\n",
      "\n",
      "Test set: Accuracy: 450/837 (54%)\n",
      "\n",
      "Train Epoch: 42 [0/7895 (0%)]\tLoss: 0.421181\n",
      "Train Epoch: 42 [1280/7895 (16%)]\tLoss: 0.591154\n",
      "Train Epoch: 42 [2560/7895 (33%)]\tLoss: 0.326164\n",
      "Train Epoch: 42 [3840/7895 (49%)]\tLoss: 0.391219\n",
      "Train Epoch: 42 [5120/7895 (66%)]\tLoss: 0.346537\n",
      "Train Epoch: 42 [6400/7895 (82%)]\tLoss: 0.456758\n",
      "Train Epoch: 42 [7680/7895 (98%)]\tLoss: 0.423237\n",
      "\n",
      "Test set: Accuracy: 478/837 (57%)\n",
      "\n",
      "Train Epoch: 43 [0/7895 (0%)]\tLoss: 0.375627\n",
      "Train Epoch: 43 [1280/7895 (16%)]\tLoss: 0.370700\n",
      "Train Epoch: 43 [2560/7895 (33%)]\tLoss: 0.480263\n",
      "Train Epoch: 43 [3840/7895 (49%)]\tLoss: 0.297571\n",
      "Train Epoch: 43 [5120/7895 (66%)]\tLoss: 0.431980\n",
      "Train Epoch: 43 [6400/7895 (82%)]\tLoss: 0.466082\n",
      "Train Epoch: 43 [7680/7895 (98%)]\tLoss: 0.333770\n",
      "\n",
      "Test set: Accuracy: 379/837 (45%)\n",
      "\n",
      "Train Epoch: 44 [0/7895 (0%)]\tLoss: 0.418744\n",
      "Train Epoch: 44 [1280/7895 (16%)]\tLoss: 0.234106\n",
      "Train Epoch: 44 [2560/7895 (33%)]\tLoss: 0.228292\n",
      "Train Epoch: 44 [3840/7895 (49%)]\tLoss: 0.384921\n",
      "Train Epoch: 44 [5120/7895 (66%)]\tLoss: 0.405972\n",
      "Train Epoch: 44 [6400/7895 (82%)]\tLoss: 0.489265\n",
      "Train Epoch: 44 [7680/7895 (98%)]\tLoss: 0.431527\n",
      "\n",
      "Test set: Accuracy: 417/837 (50%)\n",
      "\n",
      "Train Epoch: 45 [0/7895 (0%)]\tLoss: 0.308788\n",
      "Train Epoch: 45 [1280/7895 (16%)]\tLoss: 0.351287\n",
      "Train Epoch: 45 [2560/7895 (33%)]\tLoss: 0.493576\n",
      "Train Epoch: 45 [3840/7895 (49%)]\tLoss: 0.265657\n",
      "Train Epoch: 45 [5120/7895 (66%)]\tLoss: 0.260689\n",
      "Train Epoch: 45 [6400/7895 (82%)]\tLoss: 0.412746\n",
      "Train Epoch: 45 [7680/7895 (98%)]\tLoss: 0.503050\n",
      "\n",
      "Test set: Accuracy: 410/837 (49%)\n",
      "\n",
      "Train Epoch: 46 [0/7895 (0%)]\tLoss: 0.240614\n",
      "Train Epoch: 46 [1280/7895 (16%)]\tLoss: 0.503426\n",
      "Train Epoch: 46 [2560/7895 (33%)]\tLoss: 0.446025\n",
      "Train Epoch: 46 [3840/7895 (49%)]\tLoss: 0.278890\n",
      "Train Epoch: 46 [5120/7895 (66%)]\tLoss: 0.427668\n",
      "Train Epoch: 46 [6400/7895 (82%)]\tLoss: 0.400292\n",
      "Train Epoch: 46 [7680/7895 (98%)]\tLoss: 0.425656\n",
      "\n",
      "Test set: Accuracy: 435/837 (52%)\n",
      "\n",
      "Train Epoch: 47 [0/7895 (0%)]\tLoss: 0.206338\n",
      "Train Epoch: 47 [1280/7895 (16%)]\tLoss: 0.452210\n",
      "Train Epoch: 47 [2560/7895 (33%)]\tLoss: 0.281688\n",
      "Train Epoch: 47 [3840/7895 (49%)]\tLoss: 0.389520\n",
      "Train Epoch: 47 [5120/7895 (66%)]\tLoss: 0.316481\n",
      "Train Epoch: 47 [6400/7895 (82%)]\tLoss: 0.465751\n",
      "Train Epoch: 47 [7680/7895 (98%)]\tLoss: 0.459099\n",
      "\n",
      "Test set: Accuracy: 408/837 (49%)\n",
      "\n",
      "Train Epoch: 48 [0/7895 (0%)]\tLoss: 0.288771\n",
      "Train Epoch: 48 [1280/7895 (16%)]\tLoss: 0.424536\n",
      "Train Epoch: 48 [2560/7895 (33%)]\tLoss: 0.454856\n",
      "Train Epoch: 48 [3840/7895 (49%)]\tLoss: 0.548283\n",
      "Train Epoch: 48 [5120/7895 (66%)]\tLoss: 0.447672\n",
      "Train Epoch: 48 [6400/7895 (82%)]\tLoss: 0.347984\n",
      "Train Epoch: 48 [7680/7895 (98%)]\tLoss: 0.440851\n",
      "\n",
      "Test set: Accuracy: 425/837 (51%)\n",
      "\n",
      "Train Epoch: 49 [0/7895 (0%)]\tLoss: 0.228571\n",
      "Train Epoch: 49 [1280/7895 (16%)]\tLoss: 0.316384\n",
      "Train Epoch: 49 [2560/7895 (33%)]\tLoss: 0.438891\n",
      "Train Epoch: 49 [3840/7895 (49%)]\tLoss: 0.348665\n",
      "Train Epoch: 49 [5120/7895 (66%)]\tLoss: 0.267838\n",
      "Train Epoch: 49 [6400/7895 (82%)]\tLoss: 0.260289\n",
      "Train Epoch: 49 [7680/7895 (98%)]\tLoss: 0.465289\n",
      "\n",
      "Test set: Accuracy: 433/837 (52%)\n",
      "\n",
      "Train Epoch: 50 [0/7895 (0%)]\tLoss: 0.439611\n",
      "Train Epoch: 50 [1280/7895 (16%)]\tLoss: 0.381025\n",
      "Train Epoch: 50 [2560/7895 (33%)]\tLoss: 0.250883\n",
      "Train Epoch: 50 [3840/7895 (49%)]\tLoss: 0.336881\n",
      "Train Epoch: 50 [5120/7895 (66%)]\tLoss: 0.297246\n",
      "Train Epoch: 50 [6400/7895 (82%)]\tLoss: 0.292726\n",
      "Train Epoch: 50 [7680/7895 (98%)]\tLoss: 0.417016\n",
      "\n",
      "Test set: Accuracy: 467/837 (56%)\n",
      "\n",
      "Train Epoch: 51 [0/7895 (0%)]\tLoss: 0.450901\n",
      "Train Epoch: 51 [1280/7895 (16%)]\tLoss: 0.390673\n",
      "Train Epoch: 51 [2560/7895 (33%)]\tLoss: 0.452435\n",
      "Train Epoch: 51 [3840/7895 (49%)]\tLoss: 0.333401\n",
      "Train Epoch: 51 [5120/7895 (66%)]\tLoss: 0.342105\n",
      "Train Epoch: 51 [6400/7895 (82%)]\tLoss: 0.369355\n",
      "Train Epoch: 51 [7680/7895 (98%)]\tLoss: 0.555094\n",
      "\n",
      "Test set: Accuracy: 471/837 (56%)\n",
      "\n",
      "Train Epoch: 52 [0/7895 (0%)]\tLoss: 0.437454\n",
      "Train Epoch: 52 [1280/7895 (16%)]\tLoss: 0.320301\n",
      "Train Epoch: 52 [2560/7895 (33%)]\tLoss: 0.402520\n",
      "Train Epoch: 52 [3840/7895 (49%)]\tLoss: 0.401799\n",
      "Train Epoch: 52 [5120/7895 (66%)]\tLoss: 0.451734\n",
      "Train Epoch: 52 [6400/7895 (82%)]\tLoss: 0.500885\n",
      "Train Epoch: 52 [7680/7895 (98%)]\tLoss: 0.322535\n",
      "\n",
      "Test set: Accuracy: 472/837 (56%)\n",
      "\n",
      "Train Epoch: 53 [0/7895 (0%)]\tLoss: 0.396430\n",
      "Train Epoch: 53 [1280/7895 (16%)]\tLoss: 0.309760\n",
      "Train Epoch: 53 [2560/7895 (33%)]\tLoss: 0.393094\n",
      "Train Epoch: 53 [3840/7895 (49%)]\tLoss: 0.487334\n",
      "Train Epoch: 53 [5120/7895 (66%)]\tLoss: 0.366825\n",
      "Train Epoch: 53 [6400/7895 (82%)]\tLoss: 0.355898\n",
      "Train Epoch: 53 [7680/7895 (98%)]\tLoss: 0.200620\n",
      "\n",
      "Test set: Accuracy: 470/837 (56%)\n",
      "\n",
      "Train Epoch: 54 [0/7895 (0%)]\tLoss: 0.335865\n",
      "Train Epoch: 54 [1280/7895 (16%)]\tLoss: 0.279729\n",
      "Train Epoch: 54 [2560/7895 (33%)]\tLoss: 0.192986\n",
      "Train Epoch: 54 [3840/7895 (49%)]\tLoss: 0.303352\n",
      "Train Epoch: 54 [5120/7895 (66%)]\tLoss: 0.401786\n",
      "Train Epoch: 54 [6400/7895 (82%)]\tLoss: 0.677762\n",
      "Train Epoch: 54 [7680/7895 (98%)]\tLoss: 0.327259\n",
      "\n",
      "Test set: Accuracy: 439/837 (52%)\n",
      "\n",
      "Train Epoch: 55 [0/7895 (0%)]\tLoss: 0.153786\n",
      "Train Epoch: 55 [1280/7895 (16%)]\tLoss: 0.371666\n",
      "Train Epoch: 55 [2560/7895 (33%)]\tLoss: 0.245691\n",
      "Train Epoch: 55 [3840/7895 (49%)]\tLoss: 0.433500\n",
      "Train Epoch: 55 [5120/7895 (66%)]\tLoss: 0.503912\n",
      "Train Epoch: 55 [6400/7895 (82%)]\tLoss: 0.404658\n",
      "Train Epoch: 55 [7680/7895 (98%)]\tLoss: 0.345373\n",
      "\n",
      "Test set: Accuracy: 441/837 (53%)\n",
      "\n",
      "Train Epoch: 56 [0/7895 (0%)]\tLoss: 0.258579\n",
      "Train Epoch: 56 [1280/7895 (16%)]\tLoss: 0.197073\n",
      "Train Epoch: 56 [2560/7895 (33%)]\tLoss: 0.296786\n",
      "Train Epoch: 56 [3840/7895 (49%)]\tLoss: 0.351656\n",
      "Train Epoch: 56 [5120/7895 (66%)]\tLoss: 0.295042\n",
      "Train Epoch: 56 [6400/7895 (82%)]\tLoss: 0.330009\n",
      "Train Epoch: 56 [7680/7895 (98%)]\tLoss: 0.278311\n",
      "\n",
      "Test set: Accuracy: 431/837 (51%)\n",
      "\n",
      "Train Epoch: 57 [0/7895 (0%)]\tLoss: 0.275557\n",
      "Train Epoch: 57 [1280/7895 (16%)]\tLoss: 0.338619\n",
      "Train Epoch: 57 [2560/7895 (33%)]\tLoss: 0.389136\n",
      "Train Epoch: 57 [3840/7895 (49%)]\tLoss: 0.380279\n",
      "Train Epoch: 57 [5120/7895 (66%)]\tLoss: 0.275454\n",
      "Train Epoch: 57 [6400/7895 (82%)]\tLoss: 0.223052\n",
      "Train Epoch: 57 [7680/7895 (98%)]\tLoss: 0.455712\n",
      "\n",
      "Test set: Accuracy: 422/837 (50%)\n",
      "\n",
      "Train Epoch: 58 [0/7895 (0%)]\tLoss: 0.261291\n",
      "Train Epoch: 58 [1280/7895 (16%)]\tLoss: 0.446832\n",
      "Train Epoch: 58 [2560/7895 (33%)]\tLoss: 0.260856\n",
      "Train Epoch: 58 [3840/7895 (49%)]\tLoss: 0.244824\n",
      "Train Epoch: 58 [5120/7895 (66%)]\tLoss: 0.507100\n",
      "Train Epoch: 58 [6400/7895 (82%)]\tLoss: 0.417642\n",
      "Train Epoch: 58 [7680/7895 (98%)]\tLoss: 0.369348\n",
      "\n",
      "Test set: Accuracy: 449/837 (54%)\n",
      "\n",
      "Train Epoch: 59 [0/7895 (0%)]\tLoss: 0.348302\n",
      "Train Epoch: 59 [1280/7895 (16%)]\tLoss: 0.308602\n",
      "Train Epoch: 59 [2560/7895 (33%)]\tLoss: 0.291773\n",
      "Train Epoch: 59 [3840/7895 (49%)]\tLoss: 0.317906\n",
      "Train Epoch: 59 [5120/7895 (66%)]\tLoss: 0.437229\n",
      "Train Epoch: 59 [6400/7895 (82%)]\tLoss: 0.377019\n",
      "Train Epoch: 59 [7680/7895 (98%)]\tLoss: 0.412442\n",
      "\n",
      "Test set: Accuracy: 471/837 (56%)\n",
      "\n",
      "Train Epoch: 60 [0/7895 (0%)]\tLoss: 0.288588\n",
      "Train Epoch: 60 [1280/7895 (16%)]\tLoss: 0.331747\n",
      "Train Epoch: 60 [2560/7895 (33%)]\tLoss: 0.238657\n",
      "Train Epoch: 60 [3840/7895 (49%)]\tLoss: 0.338256\n",
      "Train Epoch: 60 [5120/7895 (66%)]\tLoss: 0.478820\n",
      "Train Epoch: 60 [6400/7895 (82%)]\tLoss: 0.311450\n",
      "Train Epoch: 60 [7680/7895 (98%)]\tLoss: 0.373955\n",
      "\n",
      "Test set: Accuracy: 441/837 (53%)\n",
      "\n",
      "Train Epoch: 61 [0/7895 (0%)]\tLoss: 0.211487\n",
      "Train Epoch: 61 [1280/7895 (16%)]\tLoss: 0.313879\n",
      "Train Epoch: 61 [2560/7895 (33%)]\tLoss: 0.339278\n",
      "Train Epoch: 61 [3840/7895 (49%)]\tLoss: 0.453222\n",
      "Train Epoch: 61 [5120/7895 (66%)]\tLoss: 0.302504\n",
      "Train Epoch: 61 [6400/7895 (82%)]\tLoss: 0.339583\n",
      "Train Epoch: 61 [7680/7895 (98%)]\tLoss: 0.260999\n",
      "\n",
      "Test set: Accuracy: 443/837 (53%)\n",
      "\n",
      "Train Epoch: 62 [0/7895 (0%)]\tLoss: 0.237517\n",
      "Train Epoch: 62 [1280/7895 (16%)]\tLoss: 0.292903\n",
      "Train Epoch: 62 [2560/7895 (33%)]\tLoss: 0.183052\n",
      "Train Epoch: 62 [3840/7895 (49%)]\tLoss: 0.365311\n",
      "Train Epoch: 62 [5120/7895 (66%)]\tLoss: 0.495623\n",
      "Train Epoch: 62 [6400/7895 (82%)]\tLoss: 0.337152\n",
      "Train Epoch: 62 [7680/7895 (98%)]\tLoss: 0.353398\n",
      "\n",
      "Test set: Accuracy: 463/837 (55%)\n",
      "\n",
      "Train Epoch: 63 [0/7895 (0%)]\tLoss: 0.384140\n",
      "Train Epoch: 63 [1280/7895 (16%)]\tLoss: 0.375308\n",
      "Train Epoch: 63 [2560/7895 (33%)]\tLoss: 0.319176\n",
      "Train Epoch: 63 [3840/7895 (49%)]\tLoss: 0.385760\n",
      "Train Epoch: 63 [5120/7895 (66%)]\tLoss: 0.222406\n",
      "Train Epoch: 63 [6400/7895 (82%)]\tLoss: 0.254825\n",
      "Train Epoch: 63 [7680/7895 (98%)]\tLoss: 0.090303\n",
      "\n",
      "Test set: Accuracy: 461/837 (55%)\n",
      "\n",
      "Train Epoch: 64 [0/7895 (0%)]\tLoss: 0.316967\n",
      "Train Epoch: 64 [1280/7895 (16%)]\tLoss: 0.314475\n",
      "Train Epoch: 64 [2560/7895 (33%)]\tLoss: 0.186728\n",
      "Train Epoch: 64 [3840/7895 (49%)]\tLoss: 0.404174\n",
      "Train Epoch: 64 [5120/7895 (66%)]\tLoss: 0.207047\n",
      "Train Epoch: 64 [6400/7895 (82%)]\tLoss: 0.311556\n",
      "Train Epoch: 64 [7680/7895 (98%)]\tLoss: 0.238800\n",
      "\n",
      "Test set: Accuracy: 414/837 (49%)\n",
      "\n",
      "Train Epoch: 65 [0/7895 (0%)]\tLoss: 0.235444\n",
      "Train Epoch: 65 [1280/7895 (16%)]\tLoss: 0.199725\n",
      "Train Epoch: 65 [2560/7895 (33%)]\tLoss: 0.284016\n",
      "Train Epoch: 65 [3840/7895 (49%)]\tLoss: 0.351196\n",
      "Train Epoch: 65 [5120/7895 (66%)]\tLoss: 0.294877\n",
      "Train Epoch: 65 [6400/7895 (82%)]\tLoss: 0.320108\n",
      "Train Epoch: 65 [7680/7895 (98%)]\tLoss: 0.301664\n",
      "\n",
      "Test set: Accuracy: 441/837 (53%)\n",
      "\n",
      "Train Epoch: 66 [0/7895 (0%)]\tLoss: 0.413026\n",
      "Train Epoch: 66 [1280/7895 (16%)]\tLoss: 0.320828\n",
      "Train Epoch: 66 [2560/7895 (33%)]\tLoss: 0.363218\n",
      "Train Epoch: 66 [3840/7895 (49%)]\tLoss: 0.608629\n",
      "Train Epoch: 66 [5120/7895 (66%)]\tLoss: 0.338466\n",
      "Train Epoch: 66 [6400/7895 (82%)]\tLoss: 0.406743\n",
      "Train Epoch: 66 [7680/7895 (98%)]\tLoss: 0.206650\n",
      "\n",
      "Test set: Accuracy: 401/837 (48%)\n",
      "\n",
      "Train Epoch: 67 [0/7895 (0%)]\tLoss: 0.350142\n",
      "Train Epoch: 67 [1280/7895 (16%)]\tLoss: 0.353390\n",
      "Train Epoch: 67 [2560/7895 (33%)]\tLoss: 0.292527\n",
      "Train Epoch: 67 [3840/7895 (49%)]\tLoss: 0.298801\n",
      "Train Epoch: 67 [5120/7895 (66%)]\tLoss: 0.308158\n",
      "Train Epoch: 67 [6400/7895 (82%)]\tLoss: 0.288535\n",
      "Train Epoch: 67 [7680/7895 (98%)]\tLoss: 0.445186\n",
      "\n",
      "Test set: Accuracy: 440/837 (53%)\n",
      "\n",
      "Train Epoch: 68 [0/7895 (0%)]\tLoss: 0.268476\n",
      "Train Epoch: 68 [1280/7895 (16%)]\tLoss: 0.255028\n",
      "Train Epoch: 68 [2560/7895 (33%)]\tLoss: 0.417683\n",
      "Train Epoch: 68 [3840/7895 (49%)]\tLoss: 0.446507\n",
      "Train Epoch: 68 [5120/7895 (66%)]\tLoss: 0.236087\n",
      "Train Epoch: 68 [6400/7895 (82%)]\tLoss: 0.198468\n",
      "Train Epoch: 68 [7680/7895 (98%)]\tLoss: 0.300837\n",
      "\n",
      "Test set: Accuracy: 438/837 (52%)\n",
      "\n",
      "Train Epoch: 69 [0/7895 (0%)]\tLoss: 0.206133\n",
      "Train Epoch: 69 [1280/7895 (16%)]\tLoss: 0.353053\n",
      "Train Epoch: 69 [2560/7895 (33%)]\tLoss: 0.454352\n",
      "Train Epoch: 69 [3840/7895 (49%)]\tLoss: 0.447034\n",
      "Train Epoch: 69 [5120/7895 (66%)]\tLoss: 0.409295\n",
      "Train Epoch: 69 [6400/7895 (82%)]\tLoss: 0.375800\n",
      "Train Epoch: 69 [7680/7895 (98%)]\tLoss: 0.270968\n",
      "\n",
      "Test set: Accuracy: 459/837 (55%)\n",
      "\n",
      "Train Epoch: 70 [0/7895 (0%)]\tLoss: 0.394739\n",
      "Train Epoch: 70 [1280/7895 (16%)]\tLoss: 0.302570\n",
      "Train Epoch: 70 [2560/7895 (33%)]\tLoss: 0.135269\n",
      "Train Epoch: 70 [3840/7895 (49%)]\tLoss: 0.339900\n",
      "Train Epoch: 70 [5120/7895 (66%)]\tLoss: 0.265194\n",
      "Train Epoch: 70 [6400/7895 (82%)]\tLoss: 0.246174\n",
      "Train Epoch: 70 [7680/7895 (98%)]\tLoss: 0.291193\n",
      "\n",
      "Test set: Accuracy: 421/837 (50%)\n",
      "\n",
      "Train Epoch: 71 [0/7895 (0%)]\tLoss: 0.298557\n",
      "Train Epoch: 71 [1280/7895 (16%)]\tLoss: 0.151387\n",
      "Train Epoch: 71 [2560/7895 (33%)]\tLoss: 0.469117\n",
      "Train Epoch: 71 [3840/7895 (49%)]\tLoss: 0.275625\n",
      "Train Epoch: 71 [5120/7895 (66%)]\tLoss: 0.441799\n",
      "Train Epoch: 71 [6400/7895 (82%)]\tLoss: 0.298497\n",
      "Train Epoch: 71 [7680/7895 (98%)]\tLoss: 0.439504\n",
      "\n",
      "Test set: Accuracy: 455/837 (54%)\n",
      "\n",
      "Train Epoch: 72 [0/7895 (0%)]\tLoss: 0.320879\n",
      "Train Epoch: 72 [1280/7895 (16%)]\tLoss: 0.239841\n",
      "Train Epoch: 72 [2560/7895 (33%)]\tLoss: 0.283102\n",
      "Train Epoch: 72 [3840/7895 (49%)]\tLoss: 0.259895\n",
      "Train Epoch: 72 [5120/7895 (66%)]\tLoss: 0.226728\n",
      "Train Epoch: 72 [6400/7895 (82%)]\tLoss: 0.203130\n",
      "Train Epoch: 72 [7680/7895 (98%)]\tLoss: 0.202447\n",
      "\n",
      "Test set: Accuracy: 458/837 (55%)\n",
      "\n",
      "Train Epoch: 73 [0/7895 (0%)]\tLoss: 0.184310\n",
      "Train Epoch: 73 [1280/7895 (16%)]\tLoss: 0.427204\n",
      "Train Epoch: 73 [2560/7895 (33%)]\tLoss: 0.204772\n",
      "Train Epoch: 73 [3840/7895 (49%)]\tLoss: 0.218030\n",
      "Train Epoch: 73 [5120/7895 (66%)]\tLoss: 0.462899\n",
      "Train Epoch: 73 [6400/7895 (82%)]\tLoss: 0.305768\n",
      "Train Epoch: 73 [7680/7895 (98%)]\tLoss: 0.240212\n",
      "\n",
      "Test set: Accuracy: 408/837 (49%)\n",
      "\n",
      "Train Epoch: 74 [0/7895 (0%)]\tLoss: 0.278568\n",
      "Train Epoch: 74 [1280/7895 (16%)]\tLoss: 0.408379\n",
      "Train Epoch: 74 [2560/7895 (33%)]\tLoss: 0.219187\n",
      "Train Epoch: 74 [3840/7895 (49%)]\tLoss: 0.188129\n",
      "Train Epoch: 74 [5120/7895 (66%)]\tLoss: 0.393362\n",
      "Train Epoch: 74 [6400/7895 (82%)]\tLoss: 0.251380\n",
      "Train Epoch: 74 [7680/7895 (98%)]\tLoss: 0.288094\n",
      "\n",
      "Test set: Accuracy: 398/837 (48%)\n",
      "\n",
      "Train Epoch: 75 [0/7895 (0%)]\tLoss: 0.263711\n",
      "Train Epoch: 75 [1280/7895 (16%)]\tLoss: 0.455398\n",
      "Train Epoch: 75 [2560/7895 (33%)]\tLoss: 0.182450\n",
      "Train Epoch: 75 [3840/7895 (49%)]\tLoss: 0.292971\n",
      "Train Epoch: 75 [5120/7895 (66%)]\tLoss: 0.420093\n",
      "Train Epoch: 75 [6400/7895 (82%)]\tLoss: 0.281477\n",
      "Train Epoch: 75 [7680/7895 (98%)]\tLoss: 0.245001\n",
      "\n",
      "Test set: Accuracy: 455/837 (54%)\n",
      "\n",
      "Train Epoch: 76 [0/7895 (0%)]\tLoss: 0.185594\n",
      "Train Epoch: 76 [1280/7895 (16%)]\tLoss: 0.125891\n",
      "Train Epoch: 76 [2560/7895 (33%)]\tLoss: 0.259133\n",
      "Train Epoch: 76 [3840/7895 (49%)]\tLoss: 0.296520\n",
      "Train Epoch: 76 [5120/7895 (66%)]\tLoss: 0.278780\n",
      "Train Epoch: 76 [6400/7895 (82%)]\tLoss: 0.266859\n",
      "Train Epoch: 76 [7680/7895 (98%)]\tLoss: 0.230556\n",
      "\n",
      "Test set: Accuracy: 467/837 (56%)\n",
      "\n",
      "Train Epoch: 77 [0/7895 (0%)]\tLoss: 0.256156\n",
      "Train Epoch: 77 [1280/7895 (16%)]\tLoss: 0.357428\n",
      "Train Epoch: 77 [2560/7895 (33%)]\tLoss: 0.187411\n",
      "Train Epoch: 77 [3840/7895 (49%)]\tLoss: 0.439649\n",
      "Train Epoch: 77 [5120/7895 (66%)]\tLoss: 0.524847\n",
      "Train Epoch: 77 [6400/7895 (82%)]\tLoss: 0.382203\n",
      "Train Epoch: 77 [7680/7895 (98%)]\tLoss: 0.387369\n",
      "\n",
      "Test set: Accuracy: 430/837 (51%)\n",
      "\n",
      "Train Epoch: 78 [0/7895 (0%)]\tLoss: 0.280955\n",
      "Train Epoch: 78 [1280/7895 (16%)]\tLoss: 0.386149\n",
      "Train Epoch: 78 [2560/7895 (33%)]\tLoss: 0.224633\n",
      "Train Epoch: 78 [3840/7895 (49%)]\tLoss: 0.385278\n",
      "Train Epoch: 78 [5120/7895 (66%)]\tLoss: 0.347632\n",
      "Train Epoch: 78 [6400/7895 (82%)]\tLoss: 0.558456\n",
      "Train Epoch: 78 [7680/7895 (98%)]\tLoss: 0.295801\n",
      "\n",
      "Test set: Accuracy: 407/837 (49%)\n",
      "\n",
      "Train Epoch: 79 [0/7895 (0%)]\tLoss: 0.412517\n",
      "Train Epoch: 79 [1280/7895 (16%)]\tLoss: 0.474113\n",
      "Train Epoch: 79 [2560/7895 (33%)]\tLoss: 0.309196\n",
      "Train Epoch: 79 [3840/7895 (49%)]\tLoss: 0.291627\n",
      "Train Epoch: 79 [5120/7895 (66%)]\tLoss: 0.631349\n",
      "Train Epoch: 79 [6400/7895 (82%)]\tLoss: 0.362836\n",
      "Train Epoch: 79 [7680/7895 (98%)]\tLoss: 0.477279\n",
      "\n",
      "Test set: Accuracy: 459/837 (55%)\n",
      "\n",
      "Train Epoch: 80 [0/7895 (0%)]\tLoss: 0.316443\n",
      "Train Epoch: 80 [1280/7895 (16%)]\tLoss: 0.349352\n",
      "Train Epoch: 80 [2560/7895 (33%)]\tLoss: 0.384129\n",
      "Train Epoch: 80 [3840/7895 (49%)]\tLoss: 0.378768\n",
      "Train Epoch: 80 [5120/7895 (66%)]\tLoss: 0.351096\n",
      "Train Epoch: 80 [6400/7895 (82%)]\tLoss: 0.419469\n",
      "Train Epoch: 80 [7680/7895 (98%)]\tLoss: 0.337475\n",
      "\n",
      "Test set: Accuracy: 414/837 (49%)\n",
      "\n",
      "Train Epoch: 81 [0/7895 (0%)]\tLoss: 0.191419\n",
      "Train Epoch: 81 [1280/7895 (16%)]\tLoss: 0.278869\n",
      "Train Epoch: 81 [2560/7895 (33%)]\tLoss: 0.391189\n",
      "Train Epoch: 81 [3840/7895 (49%)]\tLoss: 0.443604\n",
      "Train Epoch: 81 [5120/7895 (66%)]\tLoss: 0.340283\n",
      "Train Epoch: 81 [6400/7895 (82%)]\tLoss: 0.228918\n",
      "Train Epoch: 81 [7680/7895 (98%)]\tLoss: 0.277303\n",
      "\n",
      "Test set: Accuracy: 483/837 (58%)\n",
      "\n",
      "Train Epoch: 82 [0/7895 (0%)]\tLoss: 0.436324\n",
      "Train Epoch: 82 [1280/7895 (16%)]\tLoss: 0.393184\n",
      "Train Epoch: 82 [2560/7895 (33%)]\tLoss: 0.274857\n",
      "Train Epoch: 82 [3840/7895 (49%)]\tLoss: 0.367349\n",
      "Train Epoch: 82 [5120/7895 (66%)]\tLoss: 0.437095\n",
      "Train Epoch: 82 [6400/7895 (82%)]\tLoss: 0.300137\n",
      "Train Epoch: 82 [7680/7895 (98%)]\tLoss: 0.430562\n",
      "\n",
      "Test set: Accuracy: 439/837 (52%)\n",
      "\n",
      "Train Epoch: 83 [0/7895 (0%)]\tLoss: 0.295669\n",
      "Train Epoch: 83 [1280/7895 (16%)]\tLoss: 0.280219\n",
      "Train Epoch: 83 [2560/7895 (33%)]\tLoss: 0.313060\n",
      "Train Epoch: 83 [3840/7895 (49%)]\tLoss: 0.345768\n",
      "Train Epoch: 83 [5120/7895 (66%)]\tLoss: 0.366012\n",
      "Train Epoch: 83 [6400/7895 (82%)]\tLoss: 0.096006\n",
      "Train Epoch: 83 [7680/7895 (98%)]\tLoss: 0.425698\n",
      "\n",
      "Test set: Accuracy: 457/837 (55%)\n",
      "\n",
      "Train Epoch: 84 [0/7895 (0%)]\tLoss: 0.212027\n",
      "Train Epoch: 84 [1280/7895 (16%)]\tLoss: 0.263271\n",
      "Train Epoch: 84 [2560/7895 (33%)]\tLoss: 0.249690\n",
      "Train Epoch: 84 [3840/7895 (49%)]\tLoss: 0.264623\n",
      "Train Epoch: 84 [5120/7895 (66%)]\tLoss: 0.252857\n",
      "Train Epoch: 84 [6400/7895 (82%)]\tLoss: 0.390275\n",
      "Train Epoch: 84 [7680/7895 (98%)]\tLoss: 0.390286\n",
      "\n",
      "Test set: Accuracy: 408/837 (49%)\n",
      "\n",
      "Train Epoch: 85 [0/7895 (0%)]\tLoss: 0.343840\n",
      "Train Epoch: 85 [1280/7895 (16%)]\tLoss: 0.190463\n",
      "Train Epoch: 85 [2560/7895 (33%)]\tLoss: 0.228573\n",
      "Train Epoch: 85 [3840/7895 (49%)]\tLoss: 0.571637\n",
      "Train Epoch: 85 [5120/7895 (66%)]\tLoss: 0.358000\n",
      "Train Epoch: 85 [6400/7895 (82%)]\tLoss: 0.268763\n",
      "Train Epoch: 85 [7680/7895 (98%)]\tLoss: 0.335946\n",
      "\n",
      "Test set: Accuracy: 419/837 (50%)\n",
      "\n",
      "Train Epoch: 86 [0/7895 (0%)]\tLoss: 0.339380\n",
      "Train Epoch: 86 [1280/7895 (16%)]\tLoss: 0.502442\n",
      "Train Epoch: 86 [2560/7895 (33%)]\tLoss: 0.303487\n",
      "Train Epoch: 86 [3840/7895 (49%)]\tLoss: 0.364597\n",
      "Train Epoch: 86 [5120/7895 (66%)]\tLoss: 0.316763\n",
      "Train Epoch: 86 [6400/7895 (82%)]\tLoss: 0.379271\n",
      "Train Epoch: 86 [7680/7895 (98%)]\tLoss: 0.202007\n",
      "\n",
      "Test set: Accuracy: 454/837 (54%)\n",
      "\n",
      "Train Epoch: 87 [0/7895 (0%)]\tLoss: 0.297636\n",
      "Train Epoch: 87 [1280/7895 (16%)]\tLoss: 0.305494\n",
      "Train Epoch: 87 [2560/7895 (33%)]\tLoss: 0.215267\n",
      "Train Epoch: 87 [3840/7895 (49%)]\tLoss: 0.296518\n",
      "Train Epoch: 87 [5120/7895 (66%)]\tLoss: 0.384566\n",
      "Train Epoch: 87 [6400/7895 (82%)]\tLoss: 0.369551\n",
      "Train Epoch: 87 [7680/7895 (98%)]\tLoss: 0.415444\n",
      "\n",
      "Test set: Accuracy: 436/837 (52%)\n",
      "\n",
      "Train Epoch: 88 [0/7895 (0%)]\tLoss: 0.295498\n",
      "Train Epoch: 88 [1280/7895 (16%)]\tLoss: 0.315132\n",
      "Train Epoch: 88 [2560/7895 (33%)]\tLoss: 0.484096\n",
      "Train Epoch: 88 [3840/7895 (49%)]\tLoss: 0.269321\n",
      "Train Epoch: 88 [5120/7895 (66%)]\tLoss: 0.189392\n",
      "Train Epoch: 88 [6400/7895 (82%)]\tLoss: 0.186042\n",
      "Train Epoch: 88 [7680/7895 (98%)]\tLoss: 0.264942\n",
      "\n",
      "Test set: Accuracy: 437/837 (52%)\n",
      "\n",
      "Train Epoch: 89 [0/7895 (0%)]\tLoss: 0.380664\n",
      "Train Epoch: 89 [1280/7895 (16%)]\tLoss: 0.264311\n",
      "Train Epoch: 89 [2560/7895 (33%)]\tLoss: 0.204320\n",
      "Train Epoch: 89 [3840/7895 (49%)]\tLoss: 0.275024\n",
      "Train Epoch: 89 [5120/7895 (66%)]\tLoss: 0.283049\n",
      "Train Epoch: 89 [6400/7895 (82%)]\tLoss: 0.316974\n",
      "Train Epoch: 89 [7680/7895 (98%)]\tLoss: 0.502356\n",
      "\n",
      "Test set: Accuracy: 467/837 (56%)\n",
      "\n",
      "Train Epoch: 90 [0/7895 (0%)]\tLoss: 0.221515\n",
      "Train Epoch: 90 [1280/7895 (16%)]\tLoss: 0.324686\n",
      "Train Epoch: 90 [2560/7895 (33%)]\tLoss: 0.317789\n",
      "Train Epoch: 90 [3840/7895 (49%)]\tLoss: 0.304682\n",
      "Train Epoch: 90 [5120/7895 (66%)]\tLoss: 0.319660\n",
      "Train Epoch: 90 [6400/7895 (82%)]\tLoss: 0.367327\n",
      "Train Epoch: 90 [7680/7895 (98%)]\tLoss: 0.258966\n",
      "\n",
      "Test set: Accuracy: 408/837 (49%)\n",
      "\n",
      "Train Epoch: 91 [0/7895 (0%)]\tLoss: 0.354419\n",
      "Train Epoch: 91 [1280/7895 (16%)]\tLoss: 0.528778\n",
      "Train Epoch: 91 [2560/7895 (33%)]\tLoss: 0.309168\n",
      "Train Epoch: 91 [3840/7895 (49%)]\tLoss: 0.331154\n",
      "Train Epoch: 91 [5120/7895 (66%)]\tLoss: 0.323673\n",
      "Train Epoch: 91 [6400/7895 (82%)]\tLoss: 0.467568\n",
      "Train Epoch: 91 [7680/7895 (98%)]\tLoss: 0.343011\n",
      "\n",
      "Test set: Accuracy: 458/837 (55%)\n",
      "\n",
      "Train Epoch: 92 [0/7895 (0%)]\tLoss: 0.283101\n",
      "Train Epoch: 92 [1280/7895 (16%)]\tLoss: 0.243946\n",
      "Train Epoch: 92 [2560/7895 (33%)]\tLoss: 0.194727\n",
      "Train Epoch: 92 [3840/7895 (49%)]\tLoss: 0.330989\n",
      "Train Epoch: 92 [5120/7895 (66%)]\tLoss: 0.346857\n",
      "Train Epoch: 92 [6400/7895 (82%)]\tLoss: 0.210925\n",
      "Train Epoch: 92 [7680/7895 (98%)]\tLoss: 0.312816\n",
      "\n",
      "Test set: Accuracy: 454/837 (54%)\n",
      "\n",
      "Train Epoch: 93 [0/7895 (0%)]\tLoss: 0.262262\n",
      "Train Epoch: 93 [1280/7895 (16%)]\tLoss: 0.400115\n",
      "Train Epoch: 93 [2560/7895 (33%)]\tLoss: 0.322820\n",
      "Train Epoch: 93 [3840/7895 (49%)]\tLoss: 0.367379\n",
      "Train Epoch: 93 [5120/7895 (66%)]\tLoss: 0.339870\n",
      "Train Epoch: 93 [6400/7895 (82%)]\tLoss: 0.358865\n",
      "Train Epoch: 93 [7680/7895 (98%)]\tLoss: 0.214763\n",
      "\n",
      "Test set: Accuracy: 445/837 (53%)\n",
      "\n",
      "Train Epoch: 94 [0/7895 (0%)]\tLoss: 0.227773\n",
      "Train Epoch: 94 [1280/7895 (16%)]\tLoss: 0.476841\n",
      "Train Epoch: 94 [2560/7895 (33%)]\tLoss: 0.309520\n",
      "Train Epoch: 94 [3840/7895 (49%)]\tLoss: 0.191752\n",
      "Train Epoch: 94 [5120/7895 (66%)]\tLoss: 0.407733\n",
      "Train Epoch: 94 [6400/7895 (82%)]\tLoss: 0.410152\n",
      "Train Epoch: 94 [7680/7895 (98%)]\tLoss: 0.363030\n",
      "\n",
      "Test set: Accuracy: 467/837 (56%)\n",
      "\n",
      "Train Epoch: 95 [0/7895 (0%)]\tLoss: 0.238070\n",
      "Train Epoch: 95 [1280/7895 (16%)]\tLoss: 0.294083\n",
      "Train Epoch: 95 [2560/7895 (33%)]\tLoss: 0.388674\n",
      "Train Epoch: 95 [3840/7895 (49%)]\tLoss: 0.477633\n",
      "Train Epoch: 95 [5120/7895 (66%)]\tLoss: 0.344019\n",
      "Train Epoch: 95 [6400/7895 (82%)]\tLoss: 0.266603\n",
      "Train Epoch: 95 [7680/7895 (98%)]\tLoss: 0.518079\n",
      "\n",
      "Test set: Accuracy: 437/837 (52%)\n",
      "\n",
      "Train Epoch: 96 [0/7895 (0%)]\tLoss: 0.458402\n",
      "Train Epoch: 96 [1280/7895 (16%)]\tLoss: 0.301097\n",
      "Train Epoch: 96 [2560/7895 (33%)]\tLoss: 0.314711\n",
      "Train Epoch: 96 [3840/7895 (49%)]\tLoss: 0.218994\n",
      "Train Epoch: 96 [5120/7895 (66%)]\tLoss: 0.319287\n",
      "Train Epoch: 96 [6400/7895 (82%)]\tLoss: 0.244967\n",
      "Train Epoch: 96 [7680/7895 (98%)]\tLoss: 0.306365\n",
      "\n",
      "Test set: Accuracy: 475/837 (57%)\n",
      "\n",
      "Train Epoch: 97 [0/7895 (0%)]\tLoss: 0.311213\n",
      "Train Epoch: 97 [1280/7895 (16%)]\tLoss: 0.479379\n",
      "Train Epoch: 97 [2560/7895 (33%)]\tLoss: 0.312033\n",
      "Train Epoch: 97 [3840/7895 (49%)]\tLoss: 0.489704\n",
      "Train Epoch: 97 [5120/7895 (66%)]\tLoss: 0.249790\n",
      "Train Epoch: 97 [6400/7895 (82%)]\tLoss: 0.358205\n",
      "Train Epoch: 97 [7680/7895 (98%)]\tLoss: 0.317154\n",
      "\n",
      "Test set: Accuracy: 468/837 (56%)\n",
      "\n",
      "Train Epoch: 98 [0/7895 (0%)]\tLoss: 0.340630\n",
      "Train Epoch: 98 [1280/7895 (16%)]\tLoss: 0.259989\n",
      "Train Epoch: 98 [2560/7895 (33%)]\tLoss: 0.402401\n",
      "Train Epoch: 98 [3840/7895 (49%)]\tLoss: 0.371344\n",
      "Train Epoch: 98 [5120/7895 (66%)]\tLoss: 0.218920\n",
      "Train Epoch: 98 [6400/7895 (82%)]\tLoss: 0.188307\n",
      "Train Epoch: 98 [7680/7895 (98%)]\tLoss: 0.417887\n",
      "\n",
      "Test set: Accuracy: 413/837 (49%)\n",
      "\n",
      "Train Epoch: 99 [0/7895 (0%)]\tLoss: 0.321420\n",
      "Train Epoch: 99 [1280/7895 (16%)]\tLoss: 0.250209\n",
      "Train Epoch: 99 [2560/7895 (33%)]\tLoss: 0.335150\n",
      "Train Epoch: 99 [3840/7895 (49%)]\tLoss: 0.182063\n",
      "Train Epoch: 99 [5120/7895 (66%)]\tLoss: 0.268529\n",
      "Train Epoch: 99 [6400/7895 (82%)]\tLoss: 0.342973\n",
      "Train Epoch: 99 [7680/7895 (98%)]\tLoss: 0.240582\n",
      "\n",
      "Test set: Accuracy: 386/837 (46%)\n",
      "\n",
      "Train Epoch: 100 [0/7895 (0%)]\tLoss: 0.285061\n",
      "Train Epoch: 100 [1280/7895 (16%)]\tLoss: 0.520528\n",
      "Train Epoch: 100 [2560/7895 (33%)]\tLoss: 0.418333\n",
      "Train Epoch: 100 [3840/7895 (49%)]\tLoss: 0.486495\n",
      "Train Epoch: 100 [5120/7895 (66%)]\tLoss: 0.297266\n",
      "Train Epoch: 100 [6400/7895 (82%)]\tLoss: 0.292697\n",
      "Train Epoch: 100 [7680/7895 (98%)]\tLoss: 0.346096\n",
      "\n",
      "Test set: Accuracy: 462/837 (55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9474f9-81f4-411e-b4c9-63f6e4afc28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnAI",
   "language": "python",
   "name": "learnai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
