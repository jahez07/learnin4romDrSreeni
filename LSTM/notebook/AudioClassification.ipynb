{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37195c8-ec5e-4b00-ace1-1e252544930e",
   "metadata": {},
   "source": [
    "# **Audio Classification with LSTM and Torch Audio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e4926-a8e2-40ef-a5f2-4c071692c709",
   "metadata": {},
   "source": [
    "## **Libraries Required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e0c8fc-171a-41fe-92e2-5db6139dbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3d651-e468-4172-a29b-e3a99dabe752",
   "metadata": {},
   "source": [
    "## **About Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01645cd7-b786-4d5e-971a-0577cfaa096d",
   "metadata": {},
   "source": [
    "This dataset contains 8732 labeled sound excerpts ( <= 4s ) of urban sounds from 10 classes :`air_conditioner`, `car_horn`, `children_playing`, `dog_bark`, `drilling`, `engine_idling`, `gun_shot`, `jackhammer`, `siren` and `street_music`. The classes are drawn from the urban sound taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c539fd6e-0d2a-44ca-88da-47d2df1343ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    # Wrapper for the UrbanSound8K dataset\n",
    "    # Argument List\n",
    "    # path to the UrbanSound8K csv file\n",
    "    # path to the UrbanSound8K audio files\n",
    "    # list of folders to use in the dataset\n",
    "\n",
    "    def __init__(self, csv_path, file_path, folderList):\n",
    "        csvData = pd.read_csv(csv_path)\n",
    "        # initialize lists to hold file names, labels, and folder numbers\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        # loop through the csv entries and only add entries from folders in the folder list\n",
    "        for i in range(0, len(csvData)):\n",
    "            if csvData.iloc[i, 5] in folderList:\n",
    "                self.file_names.append(csvData.iloc[i, 0]) # extracting & appending each element from column 1- filenames\n",
    "                self.labels.append(csvData.iloc[i, 6]) # extracting & appending each element from column 7 - classID\n",
    "                self.folders.append(csvData.iloc[i, 5]) # extracting & appending elements from column 6 -folderNumber\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.folderList = folderList\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # format the file path and load the file\n",
    "        path = self.file_path + \"fold\" + str(self.folders[index]) + \"/\" + self.file_names[index]\n",
    "        sound, sample_rate = torchaudio.load(path, normalize=True)\n",
    "        soundData = torch.mean(sound, dim=0, keepdim=True)\n",
    "        tempData = torch.zeros([1, 160000])  # tempData accounts for audio clips that are too short\n",
    "\n",
    "        if soundData.numel() < 160000: # checks the number of elements in soundData tensor\n",
    "            tempData[:, :soundData.numel()] = soundData\n",
    "        else:\n",
    "            tempData = soundData[:, :160000]\n",
    "\n",
    "        soundData = tempData\n",
    "\n",
    "        mel_specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(soundData)  # (channel, n_mels, time)\n",
    "        mel_specgram_norm = (mel_specgram - mel_specgram.mean()) / mel_specgram.std()\n",
    "        mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate)(soundData)  # (channel, n_mfcc, time)\n",
    "        mfcc_norm = (mfcc - mfcc.mean()) / mfcc.std()\n",
    "        \n",
    "        feature = torch.cat([mel_specgram, mfcc], axis=1)\n",
    "        \n",
    "        return feature[0].permute(1, 0), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "976d68f8-1983-45eb-8083-0df504146efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature=5, out_feature=5, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_feature = n_feature\n",
    "\n",
    "        self.lstm = nn.LSTM(self.n_feature, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, out_feature)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x.shape (batch, seq_len, n_features)\n",
    "        l_out, l_hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # out.shape (batch, seq_len, n_hidden*direction)\n",
    "        out = self.dropout(l_out)\n",
    "\n",
    "        # out.shape (batch, out_feature)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, l_hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "300f7744-aed1-4f3f-9f04-1921a92e15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1339e96f-aced-4308-b065-819bc4fe6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    y_pred, y_target = [], []\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n",
    "        \n",
    "        pred = torch.max(output, dim=1).indices\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "        y_pred = y_pred + pred.tolist()\n",
    "        y_target = y_target + target.tolist()\n",
    "        \n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "374d2fa0-c4b6-44fb-89e9-fadde9fd6aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train set size: 7895\n",
      "Test set size: 837\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"lr\": 0.01, \n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"batch_size\": 128, \n",
    "    \"in_feature\": 168, # number of features per timesteps (e.g., MFCCs, spectrogram bins)\n",
    "    \"out_feature\": 10 # number of output units ( e.g., number of classes )\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "csv_path =  '../data/urbansound8k/UrbanSound8K.csv'\n",
    "file_path = '../data/urbansound8k/'\n",
    "\n",
    "train_set = UrbanSoundDataset(csv_path, file_path, range(1, 10))\n",
    "test_set = UrbanSoundDataset(csv_path, file_path, [10])\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ae9b-b898-4b7b-ab96-be8898ed9e31",
   "metadata": {},
   "source": [
    "1. **num_workers**\n",
    "\n",
    "    * What it means: Nu mber of subprocesses to use for data loading.\n",
    "    * `num_workers=0`: data loading is done in the main process (slower).\n",
    "    * `num_workers>0`: multiple worker processes load data in parallel (faster).\n",
    "\n",
    "ðŸ‘‰ Here itâ€™s set to 1, meaning one extra worker process will load data in parallel to the training loop.\n",
    "\n",
    "2. **pin_memory**\n",
    "\n",
    "    * What it means: If True, the data loader will copy tensors into page-locked (pinned) memory.\n",
    "\n",
    "    * Why? When using a GPU (device == 'cuda'), pinned memory allows faster and more efficient transfer of data from CPU â†’ GPU.\n",
    "\n",
    "    * Without it, transfers may be slower because regular memory can be swapped out by the OS.\n",
    "\n",
    "ðŸ‘‰ Itâ€™s only useful when training on GPU. Thatâ€™s why itâ€™s conditionally set only if device == 'cuda'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52d7489e-7c1c-4093-950a-c55bdd68de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}  # needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d7df414-c8fd-4912-b594-e71d1f887a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioLSTM(\n",
      "  (lstm): LSTM(168, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AudioLSTM(\n",
    "    n_feature=hyperparameters[\"in_feature\"],\n",
    "    out_feature=hyperparameters[\"out_feature\"])\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hyperparameters['lr'],\n",
    "    weight_decay=hyperparameters['weight_decay'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "clip = 5  # gradient clipping\n",
    "\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29a926-89fb-46c0-8387-0215d6d383e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "/home/sebdui/learn/.learnML/lib/python3.12/site-packages/torchaudio/functional/functional.py:585: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/7895 (0%)]\tLoss: 0.599352\n",
      "Train Epoch: 1 [1280/7895 (16%)]\tLoss: 0.677366\n",
      "Train Epoch: 1 [2560/7895 (33%)]\tLoss: 0.722687\n",
      "Train Epoch: 1 [3840/7895 (49%)]\tLoss: 0.673987\n",
      "Train Epoch: 1 [5120/7895 (66%)]\tLoss: 0.638204\n",
      "Train Epoch: 1 [6400/7895 (82%)]\tLoss: 0.523225\n",
      "Train Epoch: 1 [7680/7895 (98%)]\tLoss: 0.698279\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9474f9-81f4-411e-b4c9-63f6e4afc28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnAI",
   "language": "python",
   "name": "learnai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
